# M√ìDULO I ‚Äî Vectores Aleatorios y Geometr√≠a de Datos

> **Enfoque:** La distribuci√≥n conjunta como objeto geom√©trico (subespacios, elipsoides, proyecciones, m√©tricas) y la teor√≠a de concentraci√≥n de la medida.

---

## üìÇ Ruta del m√≥dulo y Recursos

| Recurso | Enlace al Repositorio |
| :--- | :--- |
| **üíª C√≥digo Fuente** | [Ir a carpeta `src/classroom/probabilidad`](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/probabilidad) |
| **üìù Ejercicios** | [Ver ejercicios](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/probabilidad/ejercicios) |
| **üí° Soluciones** | [Ver soluciones](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/probabilidad/soluciones) |

---

## üó∫Ô∏è Temario Detallado

### 1. Notaci√≥n matricial y Geometr√≠a ($\mu, \Sigma$)
* **Definiciones:** Vector de medias $\mu = \mathbb{E}[X]$ y matriz de covarianza $\Sigma = \mathbb{E}[(X-\mu)(X-\mu)^\top]$.
* **Propiedades:** Simetr√≠a, semidefinida positiva e interpretaci√≥n geom√©trica (varianza por direcciones).
* **Proyecciones:** Varianza de $u^\top X$ y m√©trica inducida.
* *Aplicaci√≥n:* Feature scaling, Covariance shift, Distancia de Mahalanobis.

### 2. Dependencia Lineal vs. Independencia Estad√≠stica
* **Conceptos:** Diferencia entre dependencia lineal (rango, singularidad) e independencia estad√≠stica ($p(x,y)=p(x)p(y)$).
* **La falacia de la correlaci√≥n:** Correlaci√≥n $\neq$ Independencia (excepto en Gaussianas).
* *Aplicaci√≥n:* Multicolinealidad en regresi√≥n, selecci√≥n de variables.

### 3. Normal Multivariante (MVN)
* **Definici√≥n:** Densidad, forma cuadr√°tica y energ√≠a.
* **Geometr√≠a:** Elipsoides de densidad y descomposici√≥n espectral ($\Sigma = U \Lambda U^\top$).
* **Whitening (Blanqueo):** Transformaci√≥n $Z=\Lambda^{-1/2}U^\top(X-\mu)$ para decorrelacionar datos.
* *Aplicaci√≥n:* PCA, Gaussian Processes, Batch Normalization.

### 4. Teor√≠a del Aprendizaje (Concentraci√≥n de la Medida)
* **Convergencia:** Ley de los Grandes N√∫meros (LLN) y Teorema Central del L√≠mite (CLT) vistos como convergencia estoc√°stica.
* **Desigualdades:**
    * **Chebyshev:** Cota b√°sica basada en varianza.
    * **Hoeffding:** Cota exponencial para variables acotadas (fundamental en ML).
* *Aplicaci√≥n:* Cotas de error en generalizaci√≥n, intervalos de confianza para m√©tricas.

---

## üìö Bibliograf√≠a Espec√≠fica

* **Wasserman, *All of Statistics*:**
    * Chapter 3: Expectation & Variance.
    * Chapter 4‚Äì5: Convergence & Inequalities (Concentration).
    * Chapter 14: Multivariate Models.
* **Hastie et al., *ESL*:**
    * Chapter 2.4‚Äì2.5: Statistical Decision Theory / Local Methods.
    * Chapter 3.5: Dimensionality Reduction.
* **Koller & Friedman, *PGM*:**
    * Chapter 2‚Äì3: Independence & Factorization.
