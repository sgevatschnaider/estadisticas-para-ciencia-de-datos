# M√ìDULO I: Probabilidad Multivariante y Geometr√≠a de Datos

<p align="center">
  <img src="../../../assets/portada_probabilidad.gif" alt="Visualizaci√≥n: Covarianza y Elipsoides" width="100%">
  <br>
  <sub style="font-size: 14px;">
    <b>√Ålgebra Lineal Aleatoria</b> &nbsp;|&nbsp;
    <b>Normal Multivariante (MVN)</b> &nbsp;|&nbsp;
    <b>Concentraci√≥n de la Medida</b>
  </sub>
</p>

---

## üéØ Objetivos de Aprendizaje (Engineering Track)

Al completar este m√≥dulo, dejar√°s de ver la probabilidad como "lanzar dados" y pasar√°s a verla como **geometr√≠a en alta dimensi√≥n**. Podr√°s:

- **Geometrica de Datos:** Entender la correlaci√≥n como el coseno del √°ngulo entre vectores aleatorios (Espacios de Hilbert).
- **Ingenier√≠a de la MVN:** Dominar la **Normal Multivariante**, la distribuci√≥n m√°s importante en Machine Learning y Filtros de Kalman.
- **Pre-procesamiento:** Implementar **Whitening (Blanqueo)** y PCA desde cero usando descomposici√≥n espectral ($\Sigma = U\Lambda U^T$).
- **Detecci√≥n de Anomal√≠as:** Utilizar la **Distancia de Mahalanobis** para encontrar outliers en dimensiones $d > 3$.
- **Garant√≠as Te√≥ricas:** Aplicar desigualdades de concentraci√≥n (**Chebyshev, Hoeffding**) para acotar errores en algoritmos de aprendizaje.

---

## üõ†Ô∏è Stack Tecnol√≥gico

Requisitos para este m√≥dulo:

- **Core:** `numpy` (√°lgebra lineal), `scipy.stats` (distribuciones).
- **Visualizaci√≥n:** `matplotlib`, `seaborn` (est√°tica), `plotly` (interactiva 3D).
- **Simulaci√≥n:** `statsmodels` (opcional).

```bash
# Instalaci√≥n del entorno
pip install numpy scipy matplotlib seaborn plotly pandas

```

---

## üß™ Laboratorio Interactivo (Notebooks & Apps)

Esta secci√≥n contiene los recursos principales. Los **Notebooks** combinan teor√≠a dura (Wasserman/Kenett) con c√≥digo, y las **Apps HTML** son simuladores visuales standalone.

| üìÑ Recurso | üì• Acceso |
| --- | --- |
| **01. La Geometr√≠a de la Aleatoriedad** <br>

<br>

<br> <details><summary><strong>Resumen:</strong> <em>(clic para expandir)</em></summary><p>Basado en <strong>Wasserman (Cap. 3)</strong>. Dejamos atr√°s la probabilidad b√°sica para entrar en el √°lgebra lineal. Visualizamos las variables aleatorias como vectores. Definimos la covarianza como un producto interno y la correlaci√≥n como un √°ngulo. Incluye simulaciones de c√≥mo la matriz de covarianza  deforma el espacio.</p></details> | [](https://www.google.com/search?q=./01_Geometria_de_la_Aleatoriedad.ipynb) [](https://www.google.com/search?q=https://colab.research.google.com/github/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/01_Geometria_de_la_Aleatoriedad.ipynb) |
| **02. La Normal Multivariante (MVN) y Whitening** <br>

<br>

<br> <details><summary><strong>Resumen:</strong> <em>(clic para expandir)</em></summary><p>El coraz√≥n del m√≥dulo. Estudiamos la densidad  de la MVN, sus isocontornos el√≠pticos y la descomposici√≥n espectral. Implementamos el proceso de <strong>"Whitening"</strong> (decorrelaci√≥n + estandarizaci√≥n) fundamental para el Deep Learning. Referencia cruzada con <strong>Kenett (Modern Statistics)</strong> para la implementaci√≥n computacional.</p></details> | [](https://www.google.com/search?q=./02_Normal_Multivariante_y_Whitening.ipynb) [](https://www.google.com/search?q=https://colab.research.google.com/github/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/02_Normal_Multivariante_y_Whitening.ipynb) |
| **APP HTML: Simulador de Elipsoides de Confianza** <br>

<br>

<br> <details><summary><strong>Resumen:</strong> <em>(clic para expandir)</em></summary><p>Visualizaci√≥n web interactiva (D3.js/Plotly). Permite manipular los valores de la matriz de covarianza  en tiempo real y observar c√≥mo rota y se estira la nube de puntos 3D. Visualiza planos de corte y distancias de Mahalanobis interactivas.</p></details> | [](https://www.google.com/search?q=./apps/simulador_mvn.html) |
| **03. Concentraci√≥n de la Medida (Teor√≠a del Aprendizaje)** <br>

<br>

<br> <details><summary><strong>Resumen:</strong> <em>(clic para expandir)</em></summary><p>¬øPor qu√© funciona el Machine Learning? Basado en <strong>Wasserman (Cap. 4 y 5)</strong>. Exploramos la Ley de los Grandes N√∫meros y, m√°s importante, las cotas de error no asint√≥ticas (Desigualdades de Hoeffding y Mill). Simulamos "la carrera de la convergencia" para ver cu√°n r√°pido convergen los estimadores en la pr√°ctica.</p></details> | [](https://www.google.com/search?q=./03_Concentracion_de_la_Medida.ipynb) [](https://www.google.com/search?q=https://colab.research.google.com/github/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/03_Concentracion_de_la_Medida.ipynb) |

---

## üìê Fundamentos Te√≥ricos (The Canon)

### 1. Vector de Medias y Matriz de Covarianza

Dada una variable aleatoria vectorial :

> **Intuici√≥n Ingenieril:**  contiene la informaci√≥n de la "forma" de los datos. Si  (Identidad), los datos son una esfera perfecta (ruido blanco isotr√≥pico).

### 2. Distancia de Mahalanobis

La distancia euclidiana miente en altas dimensiones si las variables est√°n correlacionadas. Usamos la m√©trica natural de la distribuci√≥n:

Esta distancia es **invariante a escalas y rotaciones**. En el notebook 02 veremos c√≥mo usarla para detectar anomal√≠as bancarias o de sensores.

---

## üíª Snippets de C√≥digo Esenciales

### Generaci√≥n de Datos MVN y Rotaci√≥n (Python)

```python
import numpy as np
import matplotlib.pyplot as plt

# 1. Definir par√°metros
mu = [0, 0]
sigma = [[1, 0.8], 
         [0.8, 1]]  # Alta correlaci√≥n positiva

# 2. Muestreo (Ingenier√≠a Inversa: Cholesky)
n = 1000
L = np.linalg.cholesky(sigma) # Descomposici√≥n de Cholesky
z = np.random.normal(size=(n, 2)) # Ruido blanco no correlacionado
x = z @ L.T + mu # Transformaci√≥n af√≠n: colorear el ruido

# 3. Visualizaci√≥n r√°pida
plt.scatter(x[:,0], x[:,1], alpha=0.5)
plt.title(f"MVN con correlaci√≥n {sigma[0][1]}")
plt.axis('equal')
plt.show()

```

---

## üìö Bibliograf√≠a Espec√≠fica del M√≥dulo

Este m√≥dulo se construye sobre los siguientes textos can√≥nicos (disponibles en la carpeta de referencias):

1. **[Wasserman]** *All of Statistics*
* *Cap√≠tulo 3:* Random Vectors (Base geom√©trica).
* *Cap√≠tulo 4:* Convergence (Fundamento del aprendizaje).
* *Cap√≠tulo 14:* Multivariate Models (La teor√≠a de la MVN).


2. **[Kenett & Zacks]** *Modern Statistics: A Computer-Based Approach*
* *Cap√≠tulo 2:* Data Visualization (Enfoque computacional).


3. **[Rotondi]** *Probability, Statistics and Simulation*
* *Cap√≠tulo 4:* Simulation of Random Variables (M√©todos de Monte Carlo).



---

## üó∫Ô∏è Roadmap del M√≥dulo

1. **Semana 1:** Vectores aleatorios, matrices de covarianza y visualizaci√≥n de correlaciones.
2. **Semana 2:** La Normal Multivariante a fondo. Diagonalizaci√≥n y PCA.
3. **Semana 3:** Desigualdades y l√≠mites. ¬øCu√°ntos datos necesito? (Hoeffding).
4. **Proyecto Final:** Detector de anomal√≠as basado en Mahalanobis sobre dataset real.

---

## ‚ö†Ô∏è Errores Comunes (Troubleshooting)

* **Matriz Singular:** Si `np.linalg.inv(Sigma)` falla, tu matriz de covarianza no es invertible (tienes variables colineales). *Soluci√≥n:* Usar `np.linalg.pinv` (pseudoinversa).
* **Maldici√≥n de la Dimensionalidad:** En alta dimensi√≥n, casi todos los puntos parecen "lejanos" (concentraci√≥n en la corteza). No conf√≠es en la intuici√≥n 2D/3D ciegamente.
* **Interpretaci√≥n de Correlaci√≥n:**  implica independencia **solo** si la distribuci√≥n es Gaussiana. En otros casos, puede haber dependencia no lineal.

```

### üí° Mejoras aplicadas respecto al modelo de Grafos:

1.  **Enfoque Matem√°tico/Visual:** He reemplazado los grafos por conceptos de geometr√≠a lineal (matrices, elipsoides), que es la base de este m√≥dulo.
2.  **Interacci√≥n "Dropdown":** He mantenido las etiquetas `<details>` y `<summary>` para que el README se vea limpio, pero puedas desplegar la descripci√≥n te√≥rica de cada notebook.
3.  **Enlaces a Colab:** He dejado las rutas preparadas (`.../blob/main/src/classroom/probabilidad/...`). Cuando subas los archivos `.ipynb` a esas carpetas, los botones funcionar√°n autom√°ticamente.
4.  **Secci√≥n "Snippet":** Agregu√© un ejemplo de c√≥digo real usando descomposici√≥n de Cholesky, que demuestra "Saber hacer" (Ingenier√≠a) sobre solo teor√≠a.

```
