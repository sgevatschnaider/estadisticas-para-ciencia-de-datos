# M√ìDULO I: Probabilidad Multivariante y Geometr√≠a de Datos

<p align="center">
  <img src="../../../assets/portada_probabilidad.gif" alt="Visualizaci√≥n: Covarianza y Elipsoides" width="100%">
  <br>
  <sub style="font-size: 14px;">
    <b>√Ålgebra Lineal Aleatoria</b> &nbsp;|&nbsp;
    <b>Normal Multivariante (MVN)</b> &nbsp;|&nbsp;
    <b>Concentraci√≥n de la Medida</b>
  </sub>
</p>

---

## üéØ Objetivos de Aprendizaje (Engineering Track)

Al completar este m√≥dulo, dejar√°s de ver la probabilidad como "lanzar dados" y pasar√°s a verla como **geometr√≠a en alta dimensi√≥n**. Podr√°s:

- **Geometrica de Datos:** Entender la correlaci√≥n como el coseno del √°ngulo entre vectores aleatorios (Espacios de Hilbert).
- **Ingenier√≠a de la MVN:** Dominar la **Normal Multivariante**, la distribuci√≥n m√°s importante en Machine Learning y Filtros de Kalman.
- **Pre-procesamiento:** Implementar **Whitening (Blanqueo)** y PCA desde cero usando descomposici√≥n espectral ($\Sigma = U\Lambda U^T$).
- **Detecci√≥n de Anomal√≠as:** Utilizar la **Distancia de Mahalanobis** para encontrar outliers en dimensiones $d > 3$.
- **Garant√≠as Te√≥ricas:** Aplicar desigualdades de concentraci√≥n (**Chebyshev, Hoeffding**) para acotar errores en algoritmos de aprendizaje.

---

## üõ†Ô∏è Stack Tecnol√≥gico

Requisitos para este m√≥dulo:

- **Core:** `numpy` (√°lgebra lineal), `scipy.stats` (distribuciones).
- **Visualizaci√≥n:** `matplotlib`, `seaborn` (est√°tica), `plotly` (interactiva 3D).
- **Simulaci√≥n:** `statsmodels` (opcional).

```bash
# Instalaci√≥n del entorno
pip install numpy scipy matplotlib seaborn plotly pandas

```

---

## üß™ Laboratorio Interactivo (Notebooks & Apps)

Esta secci√≥n contiene los recursos principales. Los **Notebooks** combinan teor√≠a dura (Wasserman/Kenett) con c√≥digo, y las **Apps HTML** son simuladores visuales standalone.

| üìÑ Recurso | üì• Acceso |
|---|---|
| **Variables Aleatorias y Teor√≠a de la Medida (Lecci√≥n Interactiva)** <br><br> <details><summary><strong>Resumen:</strong> <em>(haz clic para expandir/colapsar)</em></summary><p>Cuaderno te√≥rico en Python que traduce las intuiciones visuales a la formalidad matem√°tica pura. Define rigurosamente la variable aleatoria como una funci√≥n medible (pushforward), explora la descomposici√≥n de Lebesgue, transformaciones con Jacobiano, esperanza matem√°tica y geometr√≠a en el espacio L2.</p></details> | [![Ver en GitHub](https://img.shields.io/badge/Ver%20en-GitHub-blue?style=for-the-badge&logo=github)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/87ab9a48374af5483e119978bfa60eb0bf0535bd/src/classroom/probabilidad/notebooks/01_Fundamentos_Matematicos_Probabilidad.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/87ab9a48374af5483e119978bfa60eb0bf0535bd/src/classroom/probabilidad/notebooks/01_Fundamentos_Matematicos_Probabilidad.ipynb) |

| üìÑ Recurso | üì• Acceso |
|---|---|
| **Fase 1: Del Bit al Continuo (Simulador)** <br><br> <details><summary><strong>Resumen:</strong> <em>(haz clic para expandir/colapsar)</em></summary><p>Simulador interactivo que demuestra la equivalencia fundamental entre tirar una moneda (bits) y construir un n√∫mero real en el intervalo [0,1]. Rompe la intuici√≥n discreta y establece la base para entender c√≥mo secuencias infinitas definen variables continuas, un concepto vital para PRNGs y transformadas inversas.</p></details> | [![Ver en GitHub](https://img.shields.io/badge/Ver%20en-GitHub-blue?style=for-the-badge&logo=github)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Probabilidad_moneda.html) <br><br> [![Ver Simulaci√≥n Interactiva](https://img.shields.io/badge/Ver%20Simulaci%C3%B3n-Interactiva-green?style=for-the-badge&logo=blogger)](https://htmlpreview.github.io/?https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Probabilidad_moneda.html) |
| **Fase 2: Conjuntos Cil√≠ndricos (Simulador)** <br><br> <details><summary><strong>Resumen:</strong> <em>(haz clic para expandir/colapsar)</em></summary><p>Herramienta visual que explica c√≥mo fijar condiciones finitas iniciales crea "bloques" de volumen exacto (conjuntos cil√≠ndricos) en el espacio continuo. Prepara la intuici√≥n para comprender las œÉ-√°lgebras y c√≥mo se atrapan porciones de medida probabil√≠stica, simulando 10,000 secuencias en tiempo real.</p></details> | [![Ver en GitHub](https://img.shields.io/badge/Ver%20en-GitHub-blue?style=for-the-badge&logo=github)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Probabilidad_cilindros.html) <br><br> [![Ver Simulaci√≥n Interactiva](https://img.shields.io/badge/Ver%20Simulaci%C3%B3n-Interactiva-green?style=for-the-badge&logo=blogger)](https://htmlpreview.github.io/?https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Probabilidad_cilindros.html) |
| **Fase 3: La Paradoja del Cero (Simulador)** <br><br> <details><summary><strong>Resumen:</strong> <em>(haz clic para expandir/colapsar)</em></summary><p>Simulador que escala el problema del cumplea√±os al infinito para ilustrar el colapso de la probabilidad puntual. Demuestra interactivamente que el hecho de que un evento continuo tenga probabilidad de cero (medida de Lebesgue nula) no significa que sea topol√≥gicamente imposible.</p></details> | [![Ver en GitHub](https://img.shields.io/badge/Ver%20en-GitHub-blue?style=for-the-badge&logo=github)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Probabilidad_paradoja_del_cero.html) <br><br> [![Ver Simulaci√≥n Interactiva](https://img.shields.io/badge/Ver%20Simulaci%C3%B3n-Interactiva-green?style=for-the-badge&logo=blogger)](https://htmlpreview.github.io/?https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Probabilidad_paradoja_del_cero.html) |
| **Fase 4: √Årbol Binario en Disco de Poincar√© (Simulador)** <br><br> <details><summary><strong>Resumen:</strong> <em>(haz clic para expandir/colapsar)</em></summary><p>Representaci√≥n avanzada que sit√∫a el espacio muestral de secuencias como un Grafo de Cayley incrustado en geometr√≠a hiperb√≥lica. Ilustra visualmente c√≥mo el espacio muestral continuo reside en el l√≠mite infinito (el polvo de Cantor), d√°ndole una interpretaci√≥n geom√©trica rigurosa al azar.</p></details> | [![Ver en GitHub](https://img.shields.io/badge/Ver%20en-GitHub-blue?style=for-the-badge&logo=github)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Probabilidad_poincare_tree.html) <br><br> [![Ver Simulaci√≥n Interactiva](https://img.shields.io/badge/Ver%20Simulaci%C3%B3n-Interactiva-green?style=for-the-badge&logo=blogger)](https://htmlpreview.github.io/?https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Probabilidad_poincare_tree.html) |
| **Fase 5: Variables Aleatorias y Teor√≠a de la Medida (Lecci√≥n Interactiva)** <br><br> <details><summary><strong>Resumen:</strong> <em>(haz clic para expandir/colapsar)</em></summary><p>Cuaderno te√≥rico en Python que traduce las intuiciones visuales a la formalidad matem√°tica pura. Define rigurosamente la variable aleatoria como una funci√≥n medible (pushforward), explora la descomposici√≥n de Lebesgue, transformaciones con Jacobiano, esperanza matem√°tica y geometr√≠a en el espacio L2.</p></details> | [![Ver en GitHub](https://img.shields.io/badge/Ver%20en-GitHub-blue?style=for-the-badge&logo=github)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/notebooks/01_Fundamentos_Matematicos_Probabilidad.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/notebooks/01_Fundamentos_Matematicos_Probabilidad.ipynb) |
| **Fase 6: La M√°quina de la Creaci√≥n (Simulador)** <br><br> <details><summary><strong>Resumen:</strong> <em>(haz clic para expandir/colapsar)</em></summary><p>Aplicaci√≥n pr√°ctica de toda la teor√≠a anterior utilizando el Teorema de la Transformada Inversa. El simulador muestra c√≥mo una distribuci√≥n uniforme generada en [0,1] se mapea a trav√©s de la funci√≥n probit para hacer emerger la Campana de Gauss en tiempo real.</p></details> | [![Ver en GitHub](https://img.shields.io/badge/Ver%20en-GitHub-blue?style=for-the-badge&logo=github)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Probabilidad_creation_machine.html) <br><br> [![Ver Simulaci√≥n Interactiva](https://img.shields.io/badge/Ver%20Simulaci%C3%B3n-Interactiva-green?style=for-the-badge&logo=blogger)](https://htmlpreview.github.io/?https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Probabilidad_creation_machine.html) |
| **Fase 7: El Guardi√°n de las Colas (Simulador)** <br><br> <details><summary><strong>Resumen:</strong> <em>(haz clic para expandir/colapsar)</em></summary><p>Simulador interactivo sobre las desigualdades de concentraci√≥n de Markov y Chebyshev. Permite explorar visualmente c√≥mo acotar probabilidades en las colas sin conocer la distribuci√≥n subyacente, y demuestra el concepto de cota ajustada mediante un caso l√≠mite con distribuciones mixtas (Spike).</p></details> | [![Ver en GitHub](https://img.shields.io/badge/Ver%20en-GitHub-blue?style=for-the-badge&logo=github)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Probabilidad_Chebyshev_Markov.html) <br><br> [![Ver Simulaci√≥n Interactiva](https://img.shields.io/badge/Ver%20Simulaci%C3%B3n-Interactiva-green?style=for-the-badge&logo=blogger)](https://htmlpreview.github.io/?https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Probabilidad_Chebyshev_Markov.html) |
| **Fase 8: La F√°brica de √Åtomos (Simulador)** <br><br> <details><summary><strong>Resumen:</strong> <em>(haz clic para expandir/colapsar)</em></summary><p>Laboratorio visual que explica la diferencia fundamental entre censurar y truncar datos. Muestra c√≥mo colapsar valores extremos crea masas puntuales ("√°tomos") en distribuciones continuas, ilustrando intuitivamente la necesidad matem√°tica de la Teor√≠a de la Medida de Lebesgue.</p></details> | [![Ver en GitHub](https://img.shields.io/badge/Ver%20en-GitHub-blue?style=for-the-badge&logo=github)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Fabrica_de_atomos.html) <br><br> [![Ver Simulaci√≥n Interactiva](https://img.shields.io/badge/Ver%20Simulaci%C3%B3n-Interactiva-green?style=for-the-badge&logo=blogger)](https://htmlpreview.github.io/?https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Fabrica_de_atomos.html) |
| **Fase 9: El Juego de las Monedas Infinitas (Simulador)** <br><br> <details><summary><strong>Resumen:</strong> <em>(haz clic para expandir/colapsar)</em></summary><p>Simulador avanzado que contrasta las integrales de Riemann y Lebesgue. A trav√©s de la Funci√≥n de Dirichlet y la construcci√≥n de Conjuntos Cil√≠ndricos mediante lanzamientos de moneda, demuestra c√≥mo Lebesgue agrupa por preimagen para unificar el concepto de Esperanza Matem√°tica.</p></details> | [![Ver en GitHub](https://img.shields.io/badge/Ver%20en-GitHub-blue?style=for-the-badge&logo=github)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Probabilidad_lebesgue_monedas.html) <br><br> [![Ver Simulaci√≥n Interactiva](https://img.shields.io/badge/Ver%20Simulaci%C3%B3n-Interactiva-green?style=for-the-badge&logo=blogger)](https://htmlpreview.github.io/?https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/html/01_Fundamentos_Matematicos_Probabilidad_lebesgue_monedas.html) |

---

## üé• Material Audiovisual Complementario (The Video Lectures)

Para consolidar la intuici√≥n geom√©trica y las demostraciones formales de los simuladores, esta es la curadur√≠a definitiva de clases en video. Ideal para repasar conceptos complejos de topolog√≠a, teor√≠a de la medida y econometr√≠a aplicados a la ciencia de datos.

| üé¨ Tema y Concepto Clave | üë®‚Äçüè´ Canal / Autor | üîó Enlace |
| :--- | :--- | :--- |
| **Teor√≠a de la Medida e Integral de Lebesgue**<br><sub>*Complemento para entender por qu√© la integraci√≥n de Riemann falla en el continuo y c√≥mo Lebesgue agrupa por preimagen.*</sub> | **The Bright Side of Mathematics**<br><sub>*Measure Theory (Curso Completo)*</sub> | [![Ver Playlist](https://img.shields.io/badge/Playlist-YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/playlist?list=PLBh2i93oe2qvMVqAzsX1Kuv6-4fjazZ8j) |
| **Desigualdades de Concentraci√≥n**<br><sub>*La demostraci√≥n rigurosa de Markov y Chebyshev, fundamental para entender las cotas de error en algoritmos.*</sub> | **Steve Brunton**<br><sub>*Chebyshev's Inequality*</sub><br><br>**MIT OpenCourseWare**<br><sub>*Prof. John Tsitsiklis (Lecture 20)*</sub> | [![Ver Video](https://img.shields.io/badge/Video-YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/watch?v=otCHN3s52ho) <br><br> [![Ver Video](https://img.shields.io/badge/Video-YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/watch?v=nrDkb2MAwSA) |
| **M√©todo de la Transformada Inversa**<br><sub>*La matem√°tica detr√°s de la simulaci√≥n de variables aleatorias usando la funci√≥n cuantil ($F^{-1}$).*</sub> | **Ben Lambert**<br><sub>*Inverse transform sampling*</sub><br><br>**ritvikmath**<br><sub>*Inverse Transform Sampling*</sub> | [![Ver Video](https://img.shields.io/badge/Video-YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/watch?v=rnBbYsysPaU) <br><br> [![Ver Video](https://img.shields.io/badge/Video-YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/watch?v=9ixzzPQWuAY) |
| **Topolog√≠a del Continuo y Medida Cero**<br><sub>*Paradojas del espacio muestral continuo, mapeos infinitos y por qu√© probabilidad 0 $\neq$ imposible.*</sub> | **3Blue1Brown**<br><sub>*Probabilities of continuous spaces*</sub><br><br>**Vsauce**<br><sub>*How To Count Past Infinity*</sub> | [![Ver Video](https://img.shields.io/badge/Video-YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/watch?v=ZA4JkHKZM50) <br><br> [![Ver Video](https://img.shields.io/badge/Video-YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/watch?v=SrU9YDoXE88) |
| **Censura vs. Truncamiento (Econometr√≠a)**<br><sub>*Entendiendo la formaci√≥n de √°tomos (masas puntuales) en las colas, esencial para los Modelos Tobit.*</sub> | **Anders Munk-Nielsen**<br><sub>*The Tobit Model*</sub><br><br>**Jake Clifton / Ben Lambert**<br><sub>*Econometrics II (Playlist)*</sub> | [![Ver Video](https://img.shields.io/badge/Video-YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/watch?v=IwsE8Rr6l6E) <br><br> [![Ver Playlist](https://img.shields.io/badge/Playlist-YouTube-FF0000?style=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/playlist?list=PLLZ4uVeiDmR_rzu-uYl9A5BrmvTKtH_aB) |

---
## üìñ Evaluaciones y Referencias (Recursos HTML)

| üìÑ Recurso | üì• Acceso |
| ---------- | --------- |
| **HTML: Cuestionario Experto** <br><br><details><summary><strong>Resumen:</strong> <em>(clic para expandir/colapsar)</em></summary><p>Cuestionario interactivo con 20 preguntas de nivel experto dise√±adas para evaluar la comprensi√≥n profunda de la teor√≠a de la medida, probabilidad continua y desigualdades de concentraci√≥n. Cada pregunta incluye una respuesta detallada estructurada anal√≠ticamente.</p></details> | [![Abrir HTML](https://img.shields.io/badge/Abrir-HTML5-green?style=for-the-badge&logo=html5)](https://sgevatschnaider.github.io/blockchain-finanzas-descentralizadas/unidades/u02-criptoactivos-consenso-seguridad/html/Cuestionario.html) |
| **HTML: Glosario Interactivo** <br><br><details><summary><strong>Resumen:</strong> <em>(clic para expandir/colapsar)</em></summary><p>Glosario completo con definiciones detalladas de los conceptos fundamentales. Incluye un filtro de b√∫squeda en tiempo real y navegaci√≥n alfab√©tica para facilitar la consulta y el estudio de t√©rminos clave.</p></details> | [![Abrir HTML](https://img.shields.io/badge/Abrir-HTML5-green?style=for-the-badge&logo=html5)](https://sgevatschnaider.github.io/blockchain-finanzas-descentralizadas/unidades/u02-criptoactivos-consenso-seguridad/html/Glosario.html) |

---

### Cuestionario Experto: Desarrollo Anal√≠tico

<details>
<summary><strong>1) ¬øQu√© diferencia conceptual separa a Riemann de Lebesgue, y por qu√© eso importa para probabilidad moderna?</strong></summary>
<br>
<p>La diferencia fundamental reside en la estrategia de partici√≥n antes de sumar. La integral de Riemann intenta calcular el √°rea bajo una curva particionando el eje horizontal (el dominio, $x$) en peque√±os intervalos contiguos y aproximando rect√°ngulos. Cuando una funci√≥n es altamente discontinua o fractal, este enfoque colapsa porque no se puede definir un l√≠mite consistente para los rect√°ngulos.</p>

<p>Lebesgue revoluciona este concepto: en lugar de particionar el dominio, particiona el codominio (el eje vertical, $y$). Agrupa los puntos del dominio que comparten el mismo valor de la funci√≥n (preim√°genes) y simplemente mide el "tama√±o" (la medida) de ese conjunto. En probabilidad moderna, esto es vital porque una variable aleatoria es exactamente una funci√≥n sobre un espacio abstracto de resultados ($\Omega$). Calcular una esperanza matem√°tica mediante Lebesgue se traduce naturalmente en una suma ponderada: el valor de la variable multiplicado por la probabilidad (medida) del conjunto de eventos que producen ese valor.</p>
</details>

<details>
<summary><strong>2) ¬øC√≥mo se define el ‚ÄúJuego de las Esperas‚Äù con monedas y cu√°l es exactamente la variable aleatoria de inter√©s?</strong></summary>
<br>
<p>El "Juego de las Esperas" es un modelo estoc√°stico donde se lanza una moneda repetidamente hasta que aparece la primera Cara (H). La recompensa o resultado del juego es el n√∫mero total de lanzamientos requeridos.</p>
<p>Formalmente, la variable aleatoria de inter√©s, $X$, es una funci√≥n medible que mapea el espacio de secuencias infinitas de cruces y caras hacia los n√∫meros naturales ($\mathbb{N}$). Representa el "tiempo de espera" hasta el primer √©xito. Si sale Cara en el primer intento, $X=1$. Si la secuencia comienza con Cruz-Cara (TH), $X=2$. En el caso general, $X=k$ corresponde a una racha inicial de $k-1$ cruces seguidas de una cara ($T^{k-1}H$). Esta variable aleatoria sigue una distribuci√≥n de probabilidad Geom√©trica, caracterizada por modelar el n√∫mero de ensayos de Bernoulli independientes necesarios para obtener el primer √©xito.</p>
</details>

<details>
<summary><strong>3) ¬øQu√© significa en este contexto que ‚Äúcada pila de premio es un Conjunto Cil√≠ndrico‚Äù?</strong></summary>
<br>
<p>En la teor√≠a de la medida aplicada a espacios de secuencias infinitas, como $\{H,T\}^{\mathbb{N}}$, es imposible asignar una probabilidad positiva a una √∫nica secuencia infinita espec√≠fica. Para resolver esto, se construyen "conjuntos cil√≠ndricos". Un cilindro es un evento definido por la fijaci√≥n estricta de un prefijo finito de coordenadas, dejando el resto de la secuencia infinita completamente libre.</p>
<p>Cuando agrupamos todas las secuencias que nos otorgan el mismo premio $k$, estamos definiendo un conjunto cil√≠ndrico. Por ejemplo, el evento "$X=3$" agrupa a todas las secuencias infinitas que comienzan inevitablemente con el patr√≥n TTH. Todo lo que ocurre a partir del cuarto lanzamiento es irrelevante para el premio. Esta estructura topol√≥gica es crucial porque permite asignar medidas de probabilidad exactas operando √∫nicamente sobre el segmento finito conocido.</p>
</details>

<details>
<summary><strong>4) ¬øPor qu√© en el juego de esperas se obtiene $P(X=k)=2^{-k}$ con moneda justa?</strong></summary>
<br>
<p>Para que la variable aleatoria tome exactamente el valor $X=k$, debe ocurrir un evento altamente espec√≠fico: una secuencia ininterrumpida de $k-1$ cruces, terminada abruptamente por una cara en el lanzamiento n√∫mero $k$ (el patr√≥n $T^{k-1}H$).</p>
<p>Bajo el supuesto de una moneda justa (probabilidad sim√©trica $p=1/2$) y el axioma de independencia estricta entre lanzamientos, la probabilidad de una intersecci√≥n de eventos independientes es el producto de sus probabilidades individuales. Por lo tanto, la probabilidad del patr√≥n se calcula multiplicando $1/2$ repetidas veces:</p>
$$P(T^{k-1}H) = \left(\frac{1}{2}\right)^{k-1} \times \left(\frac{1}{2}\right) = \left(\frac{1}{2}\right)^k = 2^{-k}$$
<p>Esta es la medida geom√©trica exacta que corresponde a ese conjunto cil√≠ndrico particular en el espacio muestral.</p>
</details>

<details>
<summary><strong>5) ¬øC√≥mo se calcula la esperanza $\mathbb{E}[X]$ en el juego de esperas y por qu√© da exactamente 2?</strong></summary>
<br>
<p>La esperanza matem√°tica se define formalmente como la integral de la variable aleatoria respecto a su medida de probabilidad, que en un espacio discreto se reduce a una suma infinita ponderada:</p>
$$\mathbb{E}[X] = \sum_{k=1}^{\infty} k P(X=k)$$
<p>Sustituyendo la probabilidad geom√©trica hallada previamente:</p>
$$\mathbb{E}[X] = \sum_{k=1}^{\infty} k 2^{-k} = 1\left(\frac{1}{2}\right) + 2\left(\frac{1}{4}\right) + 3\left(\frac{1}{8}\right) + \dots$$
<p>Esta es una serie aritm√©tico-geom√©trica convergente. Al resolverla anal√≠ticamente, la suma converge de manera exacta al valor 2. La intuici√≥n geom√©trica es que el peso probabil√≠stico decae exponencialmente (los denominadores crecen en potencias de 2), lo cual aplasta r√°pidamente el crecimiento lineal de los premios ($k$), resultando en un promedio te√≥rico finito y manejable de 2 lanzamientos.</p>
</details>

<details>
<summary><strong>6) ¬øQu√© diferencia conceptual existe entre la ‚Äúsimulaci√≥n‚Äù de un experimento y su c√°lculo exacto anal√≠tico?</strong></summary>
<br>
<p>La simulaci√≥n computacional (m√©todos de Monte Carlo) se fundamenta en la Ley de los Grandes N√∫meros. Al repetir el experimento estoc√°stico millones de veces y registrar las frecuencias emp√≠ricas, el promedio muestral tiende a converger hacia el valor esperado te√≥rico. Es una herramienta poderosa para generar intuici√≥n o resolver integrales intratables en espacios de alta dimensionalidad.</p>
<p>Sin embargo, su limitaci√≥n conceptual radica en que nunca constituye una demostraci√≥n matem√°tica rigurosa. Una simulaci√≥n siempre arrastra un margen de error estad√≠stico (varianza emp√≠rica) y est√° supeditada a las limitaciones de los generadores de n√∫meros pseudoaleatorios (PRNG). El c√°lculo anal√≠tico exacto, ya sea mediante series discretas o la integral de Lebesgue, entrega un resultado absoluto, universal y libre de ruido estoc√°stico.</p>
</details>

<details>
<summary><strong>7) ¬øCu√°l es un ejemplo donde ‚ÄúRiemann falla‚Äù y ‚ÄúLebesgue triunfa‚Äù, y qu√© ense√±a sobre medir racionales e irracionales?</strong></summary>
<br>
<p>Un caso can√≥nico es una funci√≥n indicadora tipo Dirichlet modificada en el intervalo $[0,1]$: se define un pago de 1000 si el n√∫mero es racional ($\mathbb{Q}$) y 0 si es irracional ($\mathbb{I}$).</p>
<p>La integral de Riemann colapsa al intentar calcular la esperanza. Como entre dos racionales siempre hay un irracional y viceversa, cualquier rect√°ngulo que intente aproximar el √°rea oscilar√° ca√≥ticamente entre 0 y 1000, impidiendo la convergencia del l√≠mite de las sumas.</p>
<p>Lebesgue triunfa al ignorar la posici√≥n en el eje $x$ y mirar directamente las preim√°genes. La preimagen del pago 1000 son todos los racionales en $[0,1]$, cuya medida de Lebesgue es exactamente 0 (conjunto numerable). La preimagen del pago 0 son los irracionales, cuya medida es 1. La esperanza se calcula anal√≠ticamente: $\mathbb{E}[X] = 1000(0) + 0(1) = 0$. Esto demuestra que tener infinitos puntos no equivale a poseer masa probabil√≠stica positiva.</p>
</details>

<details>
<summary><strong>8) ¬øQu√© es exactamente la ‚ÄúParadoja del Cero‚Äù en probabilidad continua?</strong></summary>
<br>
<p>La Paradoja del Cero es una aparente contradicci√≥n l√≥gica que surge al aplicar intuiciones discretas a espacios continuos reales. Se resume en el contraste entre el hecho matem√°tico de que la probabilidad de que ocurra un evento puntual espec√≠fico es estrictamente cero ($\mathbb{P}(\{x\}) = 0$) y la realidad f√≠sica de que dicho evento, de hecho, ocurre al realizar el experimento.</p>
<p>En un modelo continuo (como elegir un real uniforme en $[0,1]$), existen infinitas opciones no numerables. El "√°rea bajo la curva" para un √∫nico punto geom√©trico carece de anchura, resultando en probabilidad nula. La resoluci√≥n de la paradoja exige comprender que "medida nula" es un concepto anal√≠tico de peso relativo frente al dominio total, no un sin√≥nimo de "imposibilidad l√≥gica" (representada exclusivamente por el conjunto vac√≠o $\emptyset$).</p>
</details>

<details>
<summary><strong>9) ¬øC√≥mo se utiliza el concepto de ‚Äúbits‚Äù para explicar la transici√≥n de un modelo discreto a uno continuo?</strong></summary>
<br>
<p>Modelar un n√∫mero continuo en el intervalo $[0,1]$ computacionalmente se logra discretiz√°ndolo mediante una cantidad $b$ de bits, lo que genera un espacio muestral de cardinalidad $N=2^b$.</p>
<p>Si se utilizan pocos bits (ej. $b=5$), el espacio tiene solo 32 valores posibles, creando una cuadr√≠cula gruesa donde las probabilidades de repetici√≥n son alt√≠simas. A medida que $b \to \infty$ (ej. 60 bits, $\sim 10^{18}$ valores), la cuadr√≠cula (granularidad $2^{-b}$) se vuelve tan fina que emula el continuo real. La transici√≥n conceptual ocurre cuando el tama√±o del espacio crece exponencialmente, haciendo que la probabilidad de extraer el mismo elemento se pulverice y colapse asint√≥ticamente hacia cero.</p>
</details>

<details>
<summary><strong>10) ¬øQu√© es el ‚Äúumbral del cumplea√±os‚Äù y por qu√© es cr√≠tico en el an√°lisis probabil√≠stico de colisiones?</strong></summary>
<br>
<p>El umbral del cumplea√±os cuantifica la sorpresiva rapidez con la que aparecen colisiones al extraer muestras aleatorias uniformes de un conjunto finito de tama√±o $N$.</p>
<p>Matem√°ticamente, la probabilidad de que al menos dos muestras colisionen alcanza el 50% mucho antes de extraer $N/2$ elementos. La expansi√≥n de Taylor revela que este umbral crece proporcionalmente a $\sqrt{N}$. En un sistema modelado con $b$ bits ($N=2^b$), el umbral escala en el orden de $\sqrt{2^b} = 2^{b/2}$. Entender esta din√°mica exponencial es cr√≠tico (por ejemplo, en criptograf√≠a y dise√±o de funciones Hash) para demostrar que en espacios inmensos las colisiones ocurren much√≠simo antes de lo que dicta la intuici√≥n humana.</p>
</details>

<details>
<summary><strong>11) ¬øPor qu√© en el l√≠mite estricto continuo ‚Äúcada punto tiene probabilidad 0‚Äù y cu√°l es la lectura correcta de este fen√≥meno?</strong></summary>
<br>
<p>En un marco continuo regido por la Teor√≠a de la Medida, el espacio muestral est√° compuesto por un continuo no numerable. Si asign√°ramos a un solo punto una probabilidad uniforme estrictamente mayor que cero (digamos $\epsilon>0$), por el axioma de aditividad numerable de Kolmogorov, la suma de las probabilidades de una infinidad de puntos tender√≠a a infinito, violando la ley inquebrantable de que la probabilidad total debe ser 1.</p>
<p>La lectura te√≥rica correcta es que la masa probabil√≠stica en el continuo no reside en los puntos aislados, sino en los intervalos medibles. Un evento de medida nula es simplemente aquel que carece de extensi√≥n geom√©trica (como el diferencial $dx$) para acumular densidad de probabilidad.</p>
</details>

<details>
<summary><strong>12) ¬øQu√© significa matem√°ticamente que la probabilidad de repetici√≥n ‚Äúcolapsa hacia el cero absoluto‚Äù al aumentar la resoluci√≥n del espacio?</strong></summary>
<br>
<p>Al fijar un n√∫mero constante de ensayos $n$ y estudiar la probabilidad de extraer dos valores id√©nticos, esta m√©trica depende inversamente del tama√±o del espacio de estados $N$.</p>
<p>Para $n=1000$, si la resoluci√≥n aumenta ($N \to \infty$), la probabilidad de colisi√≥n (regida asint√≥ticamente por $1 - e^{-n^2/2N}$) se altera. Al crecer $N$ enormemente en el denominador, el exponente tiende a 0, haciendo que $e^0 \to 1$, y la probabilidad total converja a $1 - 1 = 0$. Este "colapso" no indica imposibilidad ontol√≥gica, sino que la escala de ensayos requeridos para presenciar el evento se vuelve inalcanzable operativamente.</p>
</details>

<details>
<summary><strong>13) ¬øQu√© son las ‚Äúdesigualdades de concentraci√≥n‚Äù y por qu√© se las considera universales?</strong></summary>
<br>
<p>Las desigualdades de concentraci√≥n son teoremas anal√≠ticos que permiten acotar superiormente la probabilidad de que una variable aleatoria tome valores alejados a su esperanza matem√°tica (el comportamiento de sus colas).</p>
<p>Son consideradas herramientas universales (<i>distribution-free</i>) porque no requieren suponer que los datos sigan una distribuci√≥n Gaussiana, continua o sim√©trica. Imponen l√≠mites matem√°ticos infranqueables "en el peor de los casos" vali√©ndose √∫nicamente de momentos estad√≠sticos b√°sicos (media, varianza), actuando como garantes te√≥ricos en entornos de alta incertidumbre.</p>
</details>

<details>
<summary><strong>14) Enuncia la desigualdad de Markov, enumera sus condiciones y proporciona una interpretaci√≥n intuitiva s√≥lida.</strong></summary>
<br>
<p>La desigualdad de Markov postula que para toda variable aleatoria estrictamente no negativa ($Z \ge 0$) y toda constante $a > 0$:</p>
$$\mathbb{P}(Z \ge a) \le \frac{\mathbb{E}[Z]}{a}$$
<p>Las condiciones vitales son la no-negatividad (evita que valores negativos extremos compensen espuriamente la media) y poseer un primer momento finito. Intuitivamente: "Si el promedio de un sistema cerrado es bajo, es imposible que una gran fracci√≥n de la poblaci√≥n registre valores descomunales". Por ejemplo, si el salario medio es de \$1,000, es matem√°ticamente imposible que m√°s del 10% de la muestra gane \$10,000 o m√°s; dicha subpoblaci√≥n por s√≠ sola romper√≠a la media global.</p>
</details>

<details>
<summary><strong>15) Enuncia la desigualdad de Chebyshev, detalla sus requerimientos y c√≥mo se interpreta operando con $t=k\sigma$.</strong></summary>
<br>
<p>La desigualdad de Chebyshev afirma que para toda variable $X$ con esperanza $\mu$ y varianza finita $\mathrm{Var}(X)$, para cualquier distancia $t > 0$:</p>
$$\mathbb{P}(|X-\mu| \ge t) \le \frac{\mathrm{Var}(X)}{t^2}$$

<p>Al parametrizar la distancia en desviaciones est√°ndar ($t=k\sigma$, con $\sigma=\sqrt{\mathrm{Var}(X)}$), se obtiene:</p>
$$\mathbb{P}(|X-\mu| \ge k\sigma) \le \frac{1}{k^2}$$
<p>El corolario anal√≠tico es profundo: sin conocer la distribuci√≥n subyacente, se garantiza que la masa probabil√≠stica a m√°s de $k$ desviaciones del centro jam√°s superar√° $1/k^2$. Para $k=3$, la cota superior estricta es $1/9 \approx 11.1\%$.</p>
</details>

<details>
<summary><strong>16) ¬øQu√© significa que la cota de Chebyshev sea ‚Äúajustada‚Äù (tight) y qu√© rol juega la distribuci√≥n ‚ÄúSpike‚Äù?</strong></summary>
<br>
<p>Que una cota sea "ajustada" (<i>tight</i>) significa que no es una aproximaci√≥n holgada; existe al menos un caso patol√≥gico real donde la desigualdad es una igualdad estricta. La cota no admite mejoras matem√°ticas sin sacrificar universalidad.</p>
<p>La distribuci√≥n "Spike" (de tres √°tomos) materializa este peor escenario. Asigna una masa de $1/(2k^2)$ a los extremos $\mu-k\sigma$ y $\mu+k\sigma$, centralizando el resto ($1-1/k^2$) exactamente en $\mu$. Al evaluarla bajo Chebyshev, la masa real en las colas iguala milim√©tricamente la cota $1/k^2$, demostrando que el teorema est√° perfectamente calibrado para el nivel m√°ximo de dispersi√≥n posible.</p>
</details>

<details>
<summary><strong>17) ¬øDe qu√© manera la integral de Lebesgue interpreta y calcula una esperanza como la suma de ‚Äúpremio √ó medida del conjunto que lo produce‚Äù?</strong></summary>
<br>
<p>Lebesgue abandona el escaneo secuencial del dominio temporal (Riemann) y adopta una agregaci√≥n por niveles funcionales.</p>
<p>Identifica un "premio" $k$ en el codominio y agrupa todos los eventos at√≥micos heterog√©neos en el espacio muestral que desencadenan dicho premio. Este supra-conjunto es la preimagen $X^{-1}(\{k\})$. Acto seguido, calcula la medida total (probabilidad $\mathbb{P}$) de esta preimagen. La esperanza es simplemente la combinatoria lineal de estos estratos funcionales:</p>
$$\mathbb{E}[X] = \sum_k k \cdot \mathbb{P}(X=k)$$
<p>Integrar, bajo Lebesgue, es ponderar los valores escalares de la variable por la medida volum√©trica exacta de sus preim√°genes algebraicas.</p>
</details>

<details>
<summary><strong>18) ¬øQu√© conexi√≥n te√≥rica subyace entre el ejemplo de ‚Äúracionales vs irracionales‚Äù y el concepto central de agrupaci√≥n por niveles y preim√°genes?</strong></summary>
<br>
<p>La asignaci√≥n de pagos binarios (1000 a $\mathbb{Q}$, 0 a $\mathbb{I}$) expone el fracaso de la topolog√≠a local ante funciones con alta discontinuidad. Al ser conjuntos densos en los reales, el intento de integraci√≥n geom√©trica colapsa.</p>
<p>La teor√≠a de preim√°genes elude esta geograf√≠a fractal. La preimagen del nivel 1000 aglutina todos los racionales en un ente √∫nico (medida 0). La preimagen del nivel 0 agrupa los irracionales (medida 1). La resoluci√≥n funcional ignora la vecindad espacial de los puntos y opera exclusivamente sobre el tama√±o medible de los conjuntos preimagen, confirmando la superioridad de Lebesgue en topolog√≠as complejas.</p>
</details>

<details>
<summary><strong>19) ¬øC√≥mo se aplica la desigualdad de Markov para establecer ‚Äúgarant√≠as de sistema‚Äù y analizar la latencia en ciencias de la computaci√≥n?</strong></summary>
<br>
<p>En el dise√±o de sistemas concurrentes o an√°lisis de algoritmos aleatorizados, la distribuci√≥n subyacente de la latencia (una variable estrictamente no-negativa) suele ser opaca o de colas asim√©tricas, volviendo inaplicables los modelos Gaussianos.</p>
<p>Markov act√∫a como un blindaje estoc√°stico. Si un analista de datos conoce el tiempo medio de respuesta $\mu$, el teorema garantiza irrefutablemente que la probabilidad de sufrir un pico de latencia del orden de $10\mu$ jam√°s exceder√° el 10% de las transacciones ($\mathbb{P}(Z \ge 10\mu) \le 1/10$). Esto faculta a los ingenieros a firmar Acuerdos de Nivel de Servicio (SLAs) con certezas matem√°ticas hard-coded, libres de presunciones emp√≠ricas.</p>
</details>

<details>
<summary><strong>20) ¬øDe qu√© manera la desigualdad de Chebyshev es explotada en la ‚Äúdetecci√≥n de anomal√≠as‚Äù y cu√°l es el mensaje estad√≠stico fundamental?</strong></summary>
<br>
<p>En anal√≠tica forense o monitoreo de fraude, clasificar anomal√≠as asumiendo que los datos respetan la Regla Emp√≠rica Normal ($3\sigma = 0.3\%$) es altamente riesgoso ante distribuciones de cola pesada.</p>

<p>Chebyshev proporciona el filtro conservador definitivo. Demuestra que incluso en la topolog√≠a m√°s ca√≥tica imaginable con varianza finita, el l√≠mite absoluto de observaciones que exceden las $3\sigma$ est√° r√≠gidamente fijado en $1/3^2 \approx 11.1\%$. El mensaje estad√≠stico cardinal es el <i>dise√±o robusto</i>: la interrogante forense √≥ptima no es indagar probabilidades exactas bajo normalidad ilusoria, sino establecer la peor desviaci√≥n matem√°tica garantizable. Un dato que quiebra el umbral de Chebyshev es, innegablemente, una anomal√≠a estructural severa.</p>
</details>

---

| üìÑ Recurso | üì• Acceso |
| --- | --- |
| **01. La Geometr√≠a de la Aleatoriedad** <br>

| üìÑ Recurso | üì• Acceso |
| --- | --- |
| **01. La Geometr√≠a de la Aleatoriedad** <br>

<br>

<br> <details><summary><strong>Resumen:</strong> <em>(clic para expandir)</em></summary><p>Basado en <strong>Wasserman (Cap. 3)</strong>. Dejamos atr√°s la probabilidad b√°sica para entrar en el √°lgebra lineal. Visualizamos las variables aleatorias como vectores. Definimos la covarianza como un producto interno y la correlaci√≥n como un √°ngulo. Incluye simulaciones de c√≥mo la matriz de covarianza  deforma el espacio.</p></details> | [](https://www.google.com/search?q=./01_Geometria_de_la_Aleatoriedad.ipynb) [](https://www.google.com/search?q=https://colab.research.google.com/github/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/01_Geometria_de_la_Aleatoriedad.ipynb) |
| **02. La Normal Multivariante (MVN) y Whitening** <br>

<br>

<br> <details><summary><strong>Resumen:</strong> <em>(clic para expandir)</em></summary><p>El coraz√≥n del m√≥dulo. Estudiamos la densidad  de la MVN, sus isocontornos el√≠pticos y la descomposici√≥n espectral. Implementamos el proceso de <strong>"Whitening"</strong> (decorrelaci√≥n + estandarizaci√≥n) fundamental para el Deep Learning. Referencia cruzada con <strong>Kenett (Modern Statistics)</strong> para la implementaci√≥n computacional.</p></details> | [](https://www.google.com/search?q=./02_Normal_Multivariante_y_Whitening.ipynb) [](https://www.google.com/search?q=https://colab.research.google.com/github/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/02_Normal_Multivariante_y_Whitening.ipynb) |
| **APP HTML: Simulador de Elipsoides de Confianza** <br>

<br>

<br> <details><summary><strong>Resumen:</strong> <em>(clic para expandir)</em></summary><p>Visualizaci√≥n web interactiva (D3.js/Plotly). Permite manipular los valores de la matriz de covarianza  en tiempo real y observar c√≥mo rota y se estira la nube de puntos 3D. Visualiza planos de corte y distancias de Mahalanobis interactivas.</p></details> | [](https://www.google.com/search?q=./apps/simulador_mvn.html) |
| **03. Concentraci√≥n de la Medida (Teor√≠a del Aprendizaje)** <br>

<br>

<br> <details><summary><strong>Resumen:</strong> <em>(clic para expandir)</em></summary><p>¬øPor qu√© funciona el Machine Learning? Basado en <strong>Wasserman (Cap. 4 y 5)</strong>. Exploramos la Ley de los Grandes N√∫meros y, m√°s importante, las cotas de error no asint√≥ticas (Desigualdades de Hoeffding y Mill). Simulamos "la carrera de la convergencia" para ver cu√°n r√°pido convergen los estimadores en la pr√°ctica.</p></details> | [](https://www.google.com/search?q=./03_Concentracion_de_la_Medida.ipynb) [](https://www.google.com/search?q=https://colab.research.google.com/github/sgevatschnaider/estadisticas-para-ciencia-de-datos/blob/main/src/classroom/probabilidad/03_Concentracion_de_la_Medida.ipynb) |

---

## üìê Fundamentos Te√≥ricos (The Canon)

### 1. Vector de Medias y Matriz de Covarianza

Dada una variable aleatoria vectorial :

> **Intuici√≥n Ingenieril:**  contiene la informaci√≥n de la "forma" de los datos. Si  (Identidad), los datos son una esfera perfecta (ruido blanco isotr√≥pico).

### 2. Distancia de Mahalanobis

La distancia euclidiana miente en altas dimensiones si las variables est√°n correlacionadas. Usamos la m√©trica natural de la distribuci√≥n:

Esta distancia es **invariante a escalas y rotaciones**. En el notebook 02 veremos c√≥mo usarla para detectar anomal√≠as bancarias o de sensores.

---

## üíª Snippets de C√≥digo Esenciales

### Generaci√≥n de Datos MVN y Rotaci√≥n (Python)

```python
import numpy as np
import matplotlib.pyplot as plt

# 1. Definir par√°metros
mu = [0, 0]
sigma = [[1, 0.8], 
         [0.8, 1]]  # Alta correlaci√≥n positiva

# 2. Muestreo (Ingenier√≠a Inversa: Cholesky)
n = 1000
L = np.linalg.cholesky(sigma) # Descomposici√≥n de Cholesky
z = np.random.normal(size=(n, 2)) # Ruido blanco no correlacionado
x = z @ L.T + mu # Transformaci√≥n af√≠n: colorear el ruido

# 3. Visualizaci√≥n r√°pida
plt.scatter(x[:,0], x[:,1], alpha=0.5)
plt.title(f"MVN con correlaci√≥n {sigma[0][1]}")
plt.axis('equal')
plt.show()

```

---

## üìö Bibliograf√≠a Espec√≠fica del M√≥dulo

Este m√≥dulo se construye sobre los siguientes textos can√≥nicos (disponibles en la carpeta de referencias):

1. **[Wasserman]** *All of Statistics*
* *Cap√≠tulo 3:* Random Vectors (Base geom√©trica).
* *Cap√≠tulo 4:* Convergence (Fundamento del aprendizaje).
* *Cap√≠tulo 14:* Multivariate Models (La teor√≠a de la MVN).


2. **[Kenett & Zacks]** *Modern Statistics: A Computer-Based Approach*
* *Cap√≠tulo 2:* Data Visualization (Enfoque computacional).


3. **[Rotondi]** *Probability, Statistics and Simulation*
* *Cap√≠tulo 4:* Simulation of Random Variables (M√©todos de Monte Carlo).



---

## üó∫Ô∏è Roadmap del M√≥dulo

1. **Semana 1:** Vectores aleatorios, matrices de covarianza y visualizaci√≥n de correlaciones.
2. **Semana 2:** La Normal Multivariante a fondo. Diagonalizaci√≥n y PCA.
3. **Semana 3:** Desigualdades y l√≠mites. ¬øCu√°ntos datos necesito? (Hoeffding).
4. **Proyecto Final:** Detector de anomal√≠as basado en Mahalanobis sobre dataset real.

---
## üèÜ Proyecto Final: Detector de Anomal√≠as Multivariante

<p align="center">
  <img src="https://img.shields.io/badge/Dificultad-Avanzada-red?style=for-the-badge" alt="Dificultad">
  <img src="https://img.shields.io/badge/Herramienta-Jupyter_Notebook-F37626?style=for-the-badge&logo=jupyter" alt="Jupyter">
  <img src="https://img.shields.io/badge/Librer%C3%ADas-NumPy%20%7C%20SciPy-blue?style=for-the-badge" alt="Librer√≠as">
</p>



### üìù Descripci√≥n del Problema
En espacios de alta dimensi√≥n donde las variables est√°n correlacionadas, la distancia Euclidiana es enga√±osa. Un punto puede parecer "cercano" al centroide pero violar por completo la estructura de covarianza del sistema. 

El objetivo de este proyecto es implementar un sistema de **Detecci√≥n de Anomal√≠as (Outlier Detection)** desde cero utilizando la **Distancia de Mahalanobis**, aplic√°ndolo a un conjunto de datos real (por ejemplo, transacciones bancarias o lecturas de sensores industriales).

### üõ†Ô∏è Requisitos de Implementaci√≥n (Step-by-Step)

1. **Adquisici√≥n y Limpieza:**
   * Cargar un dataset multivariante continuo (se recomienda el *Credit Card Fraud Detection* de Kaggle o un dataset de telemetr√≠a).
   * Separar un subconjunto de datos "sanos" (inliers) para calibrar el modelo.

2. **Ajuste del Modelo Param√©trico (MVN):**
   * Calcular el vector de medias $\mu \in \mathbb{R}^d$.
   * Estimar la matriz de covarianza muestral $\Sigma \in \mathbb{R}^{d \times d}$.
   * *Control de estabilidad:* Verificar si $\Sigma$ es mal condicionada y aplicar regularizaci√≥n (Ridge/Tikhonov) o usar la pseudoinversa (`np.linalg.pinv`) si es necesario.

3. **Ingenier√≠a de Distancias:**
   * Implementar vectorizadamente el c√°lculo del cuadrado de la distancia de Mahalanobis para cada nueva observaci√≥n $x$:
     $$D_M^2(x) = (x - \mu)^T \Sigma^{-1} (x - \mu)$$

4. **Decisi√≥n Estad√≠stica y Umbrales:**
   * **Teor√≠a:** Sabiendo que $D_M^2$ sigue una distribuci√≥n $\chi^2$ (Chi-cuadrado) con $d$ grados de libertad.
   * **Pr√°ctica:** Utilizar `scipy.stats.chi2.ppf` para establecer un umbral de corte estricto (ej. $\alpha = 0.01$ o $\alpha = 0.001$). Todo punto que supere este umbral es clasificado como anomal√≠a.

5. **An√°lisis y Visualizaci√≥n:**
   * Proyectar los datos a 2D utilizando An√°lisis de Componentes Principales (PCA) calculado manualmente mediante la descomposici√≥n espectral de $\Sigma$.
   * Graficar las observaciones normales, las anomal√≠as detectadas y los isocontornos (elipsoides de confianza).

### üì¶ Entregables Esperados

* Un archivo `Proyecto_Mahalanobis.ipynb` completamente documentado.
* El c√≥digo debe estar estructurado en funciones limpias (ej. `fit_mvn(X)`, `mahalanobis_score(X_test, mu, cov)`, `predict_anomalies(scores, alpha)`).
* Un breve reporte final (en celdas Markdown) discutiendo:
  1. ¬øQu√© ventajas observaste respecto a usar simplemente una m√©trica Euclidiana?
  2. ¬øQu√© ocurre con la eficacia del detector si los datos subyacentes no son verdaderamente Gaussianos?

<details>
<summary>üí° <strong>Pista para la vectorizaci√≥n (clic para ver)</strong></summary>

Evita usar bucles `for` para calcular la distancia de miles de filas. Si `X` es tu matriz de datos centrada $(X - \mu)$ de dimensi√≥n $(n, d)$ y `InvSigma` es la matriz inversa $(d, d)$, puedes calcular todas las distancias simult√°neamente usando un producto de matrices y sumando a lo largo del eje correcto:

```python
delta = X_test - mu
# (n, d) @ (d, d) -> (n, d)
left_term = np.dot(delta, inv_sigma) 
# Producto elemento a elemento y suma por filas
D_squared = np.sum(left_term * delta, axis=1)
## ‚ö†Ô∏è Errores Comunes (Troubleshooting)

* **Matriz Singular:** Si `np.linalg.inv(Sigma)` falla, tu matriz de covarianza no es invertible (tienes variables colineales). *Soluci√≥n:* Usar `np.linalg.pinv` (pseudoinversa).
* **Maldici√≥n de la Dimensionalidad:** En alta dimensi√≥n, casi todos los puntos parecen "lejanos" (concentraci√≥n en la corteza). No conf√≠es en la intuici√≥n 2D/3D ciegamente.
* **Interpretaci√≥n de Correlaci√≥n:**  implica independencia **solo** si la distribuci√≥n es Gaussiana. En otros casos, puede haber dependencia no lineal.

```

