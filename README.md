Applied Statistics for Data Science (Engineering & Research Track)

<p align="center">
  <img src="assets/portada%20.gif" alt="VisualizaciÃ³n de conceptos: GeometrÃ­a de Datos, Inferencia, Learning, Modelos Estructurados y Grafos" width="100%">
  <br>
  <sub style="font-size: 14px;">
    ğŸ”µ <b>GeometrÃ­a & MVN</b> (MÃ³d. I) &nbsp;|&nbsp;
    ğŸŸ¢ <b>Inferencia Comp.</b> (MÃ³d. II) &nbsp;|&nbsp;
    ğŸŸ  <b>RegularizaciÃ³n & ML</b> (MÃ³d. III)
    <br>
    ğŸ”´ <b>Causalidad & Tiempo</b> (MÃ³d. IV) &nbsp;|&nbsp;
    ğŸŸ£ <b>Grafos & Complejidad</b> (MÃ³d. V)
  </sub>
</p>

---

## ğŸ“‘ Tabla de Contenidos
1. [Accesos rÃ¡pidos](#-accesos-rÃ¡pidos)
2. [DescripciÃ³n](#-descripciÃ³n)
3. [BibliografÃ­a](#-bibliografÃ­a-base-the-canon)
4. [Syllabus](#-syllabus-detallado)
5. [InstalaciÃ³n y Uso](#-uso-y-ejecuciÃ³n)

---

## ğŸ”— Accesos rÃ¡pidos

> *Acceso directo a las carpetas del repositorio con la teorÃ­a (simuladores HTML) y el cÃ³digo fuente (Notebooks).*

<p align="center">
  <a href="https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/probabilidad"><img src="https://img.shields.io/badge/MÃ“DULO_I-GeometrÃ­a-0366d6?style=for-the-badge"></a>
  <a href="https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/inferencia"><img src="https://img.shields.io/badge/MÃ“DULO_II-Inferencia-0366d6?style=for-the-badge"></a>
  <a href="https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/learning"><img src="https://img.shields.io/badge/MÃ“DULO_III-Learning-0366d6?style=for-the-badge"></a>
  <a href="https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/pgm"><img src="https://img.shields.io/badge/MÃ“DULO_IV-Estructurados-0366d6?style=for-the-badge"></a>
  <a href="https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/tiempo"><img src="https://img.shields.io/badge/MÃ“DULO_V-Grafos-0366d6?style=for-the-badge"></a>
<p align="center">
  <a href="https://sgevatschnaider.github.io/estadisticas-para-ciencia-de-datos/" target="_blank" rel="noopener">
    <img alt="Live docs â€” GitHub Pages" src="https://img.shields.io/badge/Live%20docs-GitHub%20Pages-2b3137?style=for-the-badge&logo=github" />
  </a>
  &nbsp;
  <a href="https://mybinder.org/v2/gh/sgevatschnaider/estadisticas-para-ciencia-de-datos/main" target="_blank" rel="noopener">
    <img alt="Live demos â€” Binder" src="https://img.shields.io/badge/Live%20demos-Binder-f5a250?style=for-the-badge&logo=jupyter" />
  </a>
  &nbsp;
  <a href="https://colab.research.google.com/github/sgevatschnaider/estadisticas-para-ciencia-de-datos/" target="_blank" rel="noopener">
    <img alt="Open in â€” Colab" src="https://img.shields.io/badge/Open%20in-Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white" />
  </a>
</p>

## ğŸ¯ DescripciÃ³n

Material docente de nivel **IngenierÃ­a** para Ciencia de Datos.

El curso asume conocimientos previos de estadÃ­stica clÃ¡sica (por ejemplo: ANOVA y tests de hipÃ³tesis bÃ¡sicos) y se centra en la **â€œcaja blancaâ€** de la ingenierÃ­a estadÃ­stica moderna: inferencia computacional, geometrÃ­a de los datos, regularizaciÃ³n, y modelos grÃ¡ficos probabilÃ­sticos (PGMs), con extensiÃ³n a causalidad y series temporales.

<p align="center">
  <a href="https://www.python.org/">
    <img alt="Python" src="https://img.shields.io/badge/python-3.10%2B-blue">
  </a>
  <a href="https://www.r-project.org/">
    <img alt="R" src="https://img.shields.io/badge/R-4.x-276DC3">
  </a>
  <a href="https://julialang.org/">
    <img alt="Julia" src="https://img.shields.io/badge/Julia-1.x-9558B2">
  </a>
  <a href="https://github.com/psf/black">
    <img alt="Code style: black" src="https://img.shields.io/badge/code%20style-black-000000.svg">
  </a>
  <a href="LICENSE">
    <img alt="License: MIT" src="https://img.shields.io/badge/License-MIT-blue.svg">
  </a>
</p>

---

## ğŸ“š BibliografÃ­a base (The Canon)

Las referencias bibliogrÃ¡ficas del programa se apoyan principalmente en:

- **[Wasserman]** â€” *All of Statistics: A Concise Course in Statistical Inference* (Larry Wasserman)
- **[ESL]** â€” *The Elements of Statistical Learning* (Hastie, Tibshirani & Friedman)
- **[BDA3]** â€” *Bayesian Data Analysis (3rd Ed.)* (Gelman, Carlin, et al.)
- **[Efron]** â€” *Computer Age Statistical Inference* (Efron & Hastie)
- **[Pearl]** â€” *Causality: Models, Reasoning, and Inference* (Judea Pearl)
- **[Shumway]** â€” *Time Series Analysis and Its Applications* (Shumway & Stoffer)

---

## ğŸ¯ PropÃ³sito del curso

Conectar **probabilidad multivariante + Ã¡lgebra lineal + inferencia computacional** con la **geometrÃ­a de los datos** usada hoy en **ML/IA**: reducciÃ³n de dimensiÃ³n, regularizaciÃ³n, modelos estructurados, grafos y mÃ©todos de simulaciÃ³n/optimizaciÃ³n.

---

## ğŸ—ºï¸ Syllabus detallado

# MÃ“DULO I â€” Vectores Aleatorios y GeometrÃ­a de Datos

> **Enfoque:** La distribuciÃ³n conjunta como objeto geomÃ©trico (subespacios, elipsoides, proyecciones) y la teorÃ­a de concentraciÃ³n de la medida.

---

## ğŸ“‚ Ruta del mÃ³dulo y Recursos

| Recurso | Enlace al Repositorio |
| :--- | :--- |
| **ğŸ’» CÃ³digo Fuente** | [Ir a carpeta `src/classroom/probabilidad`](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/probabilidad) |
| **ğŸ“ Ejercicios** | [Ver ejercicios](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/probabilidad/ejercicios) |
| **ğŸ’¡ Soluciones** | [Ver soluciones](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/probabilidad/soluciones) |

---

## ğŸ—ºï¸ Temario Detallado

### 1. NotaciÃ³n matricial y GeometrÃ­a ($\mu, \Sigma$)
* **Definiciones:** Vector de medias $\mu = \mathbb{E}[X]$ y matriz de covarianza $\Sigma = \mathbb{E}[(X-\mu)(X-\mu)^\top]$.
* **Propiedades:** SimetrÃ­a, semidefinida positiva e interpretaciÃ³n geomÃ©trica (varianza por direcciones).
* **Proyecciones:** Varianza de $u^\top X$ y mÃ©trica inducida.
* *AplicaciÃ³n:* Feature scaling, Covariance shift, Distancia de Mahalanobis.

### 2. Dependencia Lineal vs. Independencia EstadÃ­stica
* **Conceptos:** Diferencia entre dependencia lineal (rango, singularidad) e independencia estadÃ­stica ($p(x,y)=p(x)p(y)$).
* **La falacia de la correlaciÃ³n:** CorrelaciÃ³n $\neq$ Independencia (excepto en Gaussianas).
* *AplicaciÃ³n:* Multicolinealidad en regresiÃ³n, selecciÃ³n de variables.

### 3. Normal Multivariante (MVN)
* **DefiniciÃ³n:** Densidad, forma cuadrÃ¡tica y energÃ­a.
* **GeometrÃ­a:** Elipsoides de densidad y descomposiciÃ³n espectral ($\Sigma = U \Lambda U^\top$).
* **Whitening (Blanqueo):** TransformaciÃ³n $Z=\Lambda^{-1/2}U^\top(X-\mu)$ para decorrelacionar datos.
* *AplicaciÃ³n:* PCA, Gaussian Processes, Batch Normalization.

### 4. TeorÃ­a del Aprendizaje (ConcentraciÃ³n de la Medida)
* **Convergencia:** Ley de los Grandes NÃºmeros (LLN) y Teorema Central del LÃ­mite (CLT) vistos como convergencia estocÃ¡stica.
* **Desigualdades:**
    * **Chebyshev:** Cota bÃ¡sica basada en varianza.
    * **Hoeffding:** Cota exponencial para variables acotadas (fundamental en ML).
* *AplicaciÃ³n:* Cotas de error en generalizaciÃ³n, intervalos de confianza para mÃ©tricas.

---

## ğŸ“š BibliografÃ­a EspecÃ­fica

* **Wasserman, *All of Statistics*:**
    * Chapter 3: Expectation & Variance.
    * Chapter 4â€“5: Convergence & Inequalities (Concentration).
    * Chapter 14: Multivariate Models.
* **Hastie et al., *ESL*:**
    * Chapter 2.4â€“2.5: Statistical Decision Theory / Local Methods.
    * Chapter 3.5: Dimensionality Reduction.
* **Koller & Friedman, *PGM*:**
    * Chapter 2â€“3: Independence & Factorization.

---

### MÃ“DULO II â€” Inferencia computacional y â€œmodernaâ€

*Enfoque: superar las limitaciones de las pruebas de hipÃ³tesis clÃ¡sicas mediante optimizaciÃ³n y simulaciÃ³n.*

5. **TeorÃ­a asintÃ³tica y mÃ¡xima verosimilitud (MLE)**
* MLE como optimizaciÃ³n:  (Gradiente/Hessiano).
* Consistencia, eficiencia, InformaciÃ³n de Fisher y cota de CramÃ©râ€“Rao.
* **AplicaciÃ³n en DS/ML:** RegresiÃ³n logÃ­stica (Cross-entropy), entrenamiento de modelos probabilÃ­sticos (Naive Bayes, HMM).
* **BibliografÃ­a:** **[Wasserman, Ch. 9]**, **[Efron & Hastie, Ch. 2]**.


6. **Bootstrap y mÃ©todos de resampling**
* EstimaciÃ³n del error estÃ¡ndar sin fÃ³rmulas cerradas.
* Bootstrap paramÃ©trico vs. no paramÃ©trico. Intervalos **BCa**.
* **AplicaciÃ³n en DS/ML:** Incertidumbre en mÃ©tricas (F1, AUC), Bagging (Random Forest), *Stability selection*.
* **BibliografÃ­a:** **[Wasserman, Ch. 8]**, **[Efron & Hastie, Ch. 10â€“11]**.


7. **Tests de hipÃ³tesis en alta dimensionalidad**
* Tests de Wald, Score y Likelihood Ratio Test (LRT).
* Comparaciones mÃºltiples: Bonferroni y **False Discovery Rate (FDR â€” Benjaminiâ€“Hochberg)**.
* **AplicaciÃ³n en DS/ML:** A/B testing a escala, selecciÃ³n de features en alta dimensiÃ³n (genes, texto).
* **BibliografÃ­a:** **[Wasserman, Ch. 10]**, **[ESL, High-Dim Problems]**, **[Efron & Hastie, Ch. 15]**.



---

### MÃ“DULO III â€” Aprendizaje estadÃ­stico (regresiÃ³n avanzada)

*Enfoque: trade-off sesgoâ€“varianza, geometrÃ­a de proyecciones y selecciÃ³n de modelos.*

8. **GeometrÃ­a de mÃ­nimos cuadrados (OLS)**
* RegresiÃ³n como proyecciÃ³n ortogonal: Matriz sombrero .
* Teorema de Gaussâ€“Markov y diagnÃ³stico (leverage, distancia de Cook).
* **AplicaciÃ³n en DS/ML:** Baselines interpretables, detecciÃ³n de puntos influyentes en producciÃ³n.
* **BibliografÃ­a:** **[ESL, Ch. 3.2]**, **[Wasserman, Ch. 13]**.


9. **RegularizaciÃ³n y selecciÃ³n de modelos**
* MaldiciÃ³n de la dimensionalidad ().
* **Ridge (L2):** ContracciÃ³n y priors gaussianos.
* **Lasso (L1):** Sparsity y priors de Laplace. Elastic Net.
* Criterios de informaciÃ³n: AIC, BIC, .
* **AplicaciÃ³n en DS/ML:** Estabilidad en embeddings, selecciÃ³n automÃ¡tica de variables, *Early stopping*.
* **BibliografÃ­a:** **[ESL, Ch. 3.4 y Ch. 7]**, **[Efron & Hastie, Ch. 7 y 16]**.


10. **Modelos lineales generalizados (GLM)**
* Familia exponencial y funciones de enlace (*link*): LogÃ­stica y Poisson.
* Algoritmo IRLS (Iteratively Reweighted Least Squares).
* **AplicaciÃ³n en DS/ML:** ClasificaciÃ³n calibrada, modelado de conteos/demanda.
* **BibliografÃ­a:** **[ESL, Ch. 4.4]**, **[Efron & Hastie, Ch. 8]**.



---

### MÃ“DULO IV â€” Modelos estructurados, grafos, causalidad y tiempo

*Enfoque: modelar dependencias complejas, inferencia bayesiana y dinÃ¡mica.*

11. **Probabilistic Graphical Models (PGMs)**
* **DAGs:** FactorizaciÃ³n de la conjunta, independencia condicional y **d-separation**.
* *Plate notation* para modelos jerÃ¡rquicos.
* **AplicaciÃ³n en DS/ML:** Naive Bayes, HMMs, modelos jerÃ¡rquicos multitenant.
* **BibliografÃ­a:** **[Koller & Friedman, Ch. 2â€“4]**, **[Bishop, Ch. 8]**.


12. **Inferencia causal**
* CorrelaciÃ³n vs. CausaciÃ³n. Intervenciones y operador .
* Confounders, colliders y criterio *back-door*.
* **AplicaciÃ³n en DS/ML:** A/B testing, Uplift modeling, eliminaciÃ³n de sesgos en datos.
* **BibliografÃ­a:** **[Pearl, Ch. 1â€“3]**, **[Wasserman, Ch. 16â€“17]**.


13. **Inferencia bayesiana y MCMC**
* Priors conjugados vs. no informativos. Posterior como creencia.
* **MCMC:** Metropolisâ€“Hastings y diagnÃ³stico bÃ¡sico.
* **AplicaciÃ³n en DS/ML:** Bayesian Logistic Regression, Bayesian Optimization, cuantificaciÃ³n de incertidumbre.
* **BibliografÃ­a:** **[BDA3, Ch. 1â€“3]**, **[Efron & Hastie, Ch. 13]**.


14. **Series temporales y modelos dinÃ¡micos**
* Estacionariedad, autocorrelaciÃ³n y modelos ARIMA.
* **State Space Models (SSM):** Filtro de Kalman.
* **AplicaciÃ³n en DS/ML:** Forecasting (demanda, finanzas), tracking (IoT), comparaciÃ³n con RNNs.
* **BibliografÃ­a:** **[Shumway & Stoffer, Ch. 1â€“3 y 6]**, **[Bishop, Ch. 13]**.



---

### MÃ“DULO V â€” GeometrÃ­a moderna: grafos, complejidad y bÃºsqueda

*Enfoque: expandir la geometrÃ­a mÃ¡s allÃ¡ de lo euclÃ­deo (grafos), dinÃ¡mica local (autÃ³matas) y optimizaciÃ³n no convexa.*

15. **TeorÃ­a de grafos para ciencia de datos**
* Matrices de adyacencia, conectividad, caminos, BFS/DFS.
* **AplicaciÃ³n en DS/ML:** Network science (PageRank, comunidades), recomendadores, detecciÃ³n de fraude.
* **BibliografÃ­a:** **[Benjaminâ€“Chartrandâ€“Zhang]**, **[Kumar]**.


16. **Espectros de grafos y geometrÃ­a (Spectral Graph Theory)**
* Espectro del Laplaciano; autovalores y autovectores en grafos.
* **AplicaciÃ³n en DS/ML:** Spectral clustering, Graph Embeddings, fundamentos de GNNs.
* **BibliografÃ­a:** **[Kumar]**, **[ESL/Bishop (ref. espectral)]**.


17. **Modelos sobre grafos y unificaciÃ³n**
* Markov blankets, MRF (Markon Random Fields) y Factor Graphs.
* Inferencia aproximada (Loopy Belief Propagation).
* **AplicaciÃ³n en DS/ML:** CRFs (secuencias), segmentaciÃ³n de imÃ¡genes, denoising.
* **BibliografÃ­a:** **[Koller & Friedman]**, **[Bishop, Ch. 8]**.


18. **Algoritmos evolutivos y programaciÃ³n genÃ©tica (GP)**
* OptimizaciÃ³n sin gradiente en paisajes no convexos.
* GP: BÃºsqueda en espacios de programas (*Symbolic Regression*).
* **AplicaciÃ³n en DS/ML:** AutoML, descubrimiento de ecuaciones interpretables, Neuroevolution.
* **BibliografÃ­a:** **[Material de CÃ¡tedra: Evolutivos/GP]**.


19. **AutÃ³matas celulares (CA) y dinÃ¡mica local**
* Reglas locales â†’ Emergencia global. DinÃ¡mica en grillas.
* **AplicaciÃ³n en DS/ML:** SimulaciÃ³n de propagaciÃ³n (epidemias, trÃ¡fico), modelos generativos discretos.
* **BibliografÃ­a:** **[Material de CÃ¡tedra: AutÃ³matas Celulares]**, **[Wolfram (ref. conceptual)]**.

##  CÃ³mo utilizar este material (InstalaciÃ³n y Uso)

Tienes dos formas principales de consumir y ejecutar las clases de este repositorio:

### OpciÃ³n A: En la Nube (Recomendado)
No necesitas instalar nada en tu computadora.
1. **Simuladores Interactivos:** Haz clic en los botones azules `[![Ver en GitHub]]` de la tabla superior para acceder a los archivos `.html`. Puedes verlos funcionar directamente usando los botones verdes de `[![Ver SimulaciÃ³n Interactiva]]`.
2. **Jupyter Notebooks:** Haz clic en los botones de **Binder** o **Colab** que se encuentran en la parte superior. Esto abrirÃ¡ un entorno virtual en tu navegador donde podrÃ¡s ejecutar el cÃ³digo Python celda por celda.

### OpciÃ³n B: EjecuciÃ³n Local
Si prefieres tener los archivos en tu mÃ¡quina:
1. Clona este repositorio:
   ```bash
   git clone [https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos.git](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos.git)

---

## ğŸ“š BibliografÃ­a General del Curso

* **Wasserman** â€” *All of Statistics*.
* **Hastie, Tibshirani, Friedman** â€” *The Elements of Statistical Learning (ESL)*.
* **Efron & Hastie** â€” *Computer Age Statistical Inference (CASI)*.
* **Bishop** â€” *Pattern Recognition and Machine Learning (PRML)*.
* **Koller & Friedman** â€” *Probabilistic Graphical Models*.
* **Gelman et al.** â€” *Bayesian Data Analysis (BDA3)*.
* **Pearl** â€” *Causal Inference in Statistics*.
* **Shumway & Stoffer** â€” *Time Series Analysis and Its Applications*.
* **Mardia, Kent, Bibby** â€” *Multivariate Analysis*.
* **Benjamin, Chartrand, Zhang** â€” *The Fascinating World of Graph Theory*.

## ğŸ§± Estructura del repositorio (high-level)

> *AjustÃ¡ estos nombres si difieren en tu repo.*

- `docs/` â€” sitio MkDocs / GitHub Pages (material por mÃ³dulo)
- `src/classroom/` â€” notebooks, scripts y prÃ¡cticas por mÃ³dulo
- `assets/` â€” imÃ¡genes, gifs y recursos visuales del curso
- `tests/` â€” tests (si aplica)
- `requirements.txt` / `pyproject.toml` â€” dependencias

---

## ğŸ› ï¸ Uso y ejecuciÃ³n

### 1) Clonar el repositorio

```bash
git clone [https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos.git](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos.git)
cd estadisticas-para-ciencia-de-datos

```

### 2) Crear entorno e instalar dependencias (Python)

Se recomienda usar un entorno virtual para evitar conflictos.

```bash
# Crear entorno virtual
python -m venv .venv

# Activar entorno (Linux/macOS):
source .venv/bin/activate

# Activar entorno (Windows):
# .venv\Scripts\activate

# Instalar dependencias
pip install -U pip
pip install -r requirements.txt

```

### 3) Calidad de cÃ³digo (opcional, recomendado)

```bash
pre-commit install
pre-commit run --all-files

```

### 4) Ejecutar notebooks / prÃ¡cticas

Si trabajÃ¡s con **Jupyter Lab**:

```bash
jupyter lab

```

Si usÃ¡s **VSCode**:

> Abre la carpeta del repositorio y ejecuta los archivos `.ipynb` directamente usando la extensiÃ³n de Jupyter.

---

## âœ… Requisitos y nivel esperado

* **MatemÃ¡tica:**
* Probabilidad y estadÃ­stica clÃ¡sica (intervalos, tests bÃ¡sicos, ANOVA).
* Ãlgebra lineal (vectores, matrices, autovalores/autovectores).


* **ProgramaciÃ³n:**
* **Python** (lenguaje principal).
* *R / Julia* (como soporte opcional).



## ğŸ¤ Contribuciones

Issues y Pull Requests son bienvenidos.

* Si vas a proponer cambios grandes (estructura, syllabus o tooling), por favor abrÃ­ primero un **Issue** describiendo el objetivo y el impacto.

## âš–ï¸ Licencia

Este material se distribuye bajo licencia **MIT**. Ver el archivo [LICENSE](https://www.google.com/search?q=LICENSE) para mÃ¡s detalles.

## ğŸ“Œ Referencias (Citas cortas)

* **[Wasserman]** Larry Wasserman â€” *All of Statistics*
* **[ESL]** Hastie, Tibshirani, Friedman â€” *The Elements of Statistical Learning*
* **[BDA3]** Gelman et al. â€” *Bayesian Data Analysis (3rd Ed.)*
* **[Efron]** Efron & Hastie â€” *Computer Age Statistical Inference*
* **[Pearl]** Judea Pearl â€” *Causality*
* **[Shumway]** Shumway & Stoffer â€” *Time Series Analysis and Its Applications*

