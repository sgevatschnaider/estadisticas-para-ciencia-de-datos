# Applied Statistics for Data Science (Engineering & Research Track)

<p align="center">
  <img src="assets/portada%20.gif" alt="VisualizaciÃ³n de conceptos: MVN, Bootstrap, RegularizaciÃ³n, Grafos y Series Temporales" width="100%">
  <br>
  <sub style="font-size: 14px;">
    ğŸ”µ <b>Normal Multivariante</b> (GeometrÃ­a) &nbsp;|&nbsp;
    ğŸŸ¢ <b>Bootstrap</b> (Inferencia) &nbsp;|&nbsp;
    ğŸŸ  <b>RegularizaciÃ³n</b> (Sparsity)
    <br>
    ğŸ”´ <b>DAGs</b> (Causalidad) &nbsp;|&nbsp;
    ğŸŸ£ <b>Series Temporales</b> (Procesos EstocÃ¡sticos)
  </sub>
</p>

---

## ğŸ”— Accesos rÃ¡pidos

### âœ… PÃ¡ginas del curso (GitHub Pages) â€” una por mÃ³dulo

[![MÃ“DULO I â€” Probabilidad](https://img.shields.io/badge/M%C3%93DULO%20I-probabilidad-0366d6)](https://sgevatschnaider.github.io/estadisticas-para-ciencia-de-datos/modulos/probabilidad/)
[![MÃ“DULO II â€” Inferencia](https://img.shields.io/badge/M%C3%93DULO%20II-inferencia-0366d6)](https://sgevatschnaider.github.io/estadisticas-para-ciencia-de-datos/modulos/inferencia/)
[![MÃ“DULO III â€” Learning](https://img.shields.io/badge/M%C3%93DULO%20III-learning-0366d6)](https://sgevatschnaider.github.io/estadisticas-para-ciencia-de-datos/modulos/learning/)
[![MÃ“DULO IV â€” PGM](https://img.shields.io/badge/M%C3%93DULO%20IV-pgm-0366d6)](https://sgevatschnaider.github.io/estadisticas-para-ciencia-de-datos/modulos/pgm/)
[![MÃ“DULO V â€” Tiempo](https://img.shields.io/badge/M%C3%93DULO%20V-tiempo-0366d6)](https://sgevatschnaider.github.io/estadisticas-para-ciencia-de-datos/modulos/tiempo/)

### ğŸ§© CÃ³digo del curso (por mÃ³dulo)

[![src/classroom/probabilidad](https://img.shields.io/badge/src-probabilidad-0366d6)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/probabilidad)
[![src/classroom/inferencia](https://img.shields.io/badge/src-inferencia-0366d6)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/inferencia)
[![src/classroom/learning](https://img.shields.io/badge/src-learning-0366d6)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/learning)
[![src/classroom/pgm](https://img.shields.io/badge/src-pgm-0366d6)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/pgm)
[![src/classroom/tiempo](https://img.shields.io/badge/src-tiempo-0366d6)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/tiempo)

<p align="center">
  <a href="https://sgevatschnaider.github.io/estadisticas-para-ciencia-de-datos/" target="_blank" rel="noopener">
    <img alt="Live docs â€” GitHub Pages" src="https://img.shields.io/badge/Live%20docs-GitHub%20Pages-2b3137?style=for-the-badge&logo=github" />
  </a>
  &nbsp;
  <a href="#" target="_blank" rel="noopener">
    <img alt="Live demos â€” Binder" src="https://img.shields.io/badge/Live%20demos-Binder-f5a250?style=for-the-badge&logo=jupyter" />
  </a>
</p>

---

## ğŸ¯ DescripciÃ³n

Material docente de nivel **IngenierÃ­a** para Ciencia de Datos.

El curso asume conocimientos previos de estadÃ­stica clÃ¡sica (por ejemplo: ANOVA y tests de hipÃ³tesis bÃ¡sicos) y se centra en la **â€œcaja blancaâ€** de la ingenierÃ­a estadÃ­stica moderna: inferencia computacional, geometrÃ­a de los datos, regularizaciÃ³n, y modelos grÃ¡ficos probabilÃ­sticos (PGMs), con extensiÃ³n a causalidad y series temporales.

<p align="center">
  <a href="https://www.python.org/">
    <img alt="Python" src="https://img.shields.io/badge/python-3.10%2B-blue">
  </a>
  <a href="https://www.r-project.org/">
    <img alt="R" src="https://img.shields.io/badge/R-4.x-276DC3">
  </a>
  <a href="https://julialang.org/">
    <img alt="Julia" src="https://img.shields.io/badge/Julia-1.x-9558B2">
  </a>
  <a href="https://github.com/psf/black">
    <img alt="Code style: black" src="https://img.shields.io/badge/code%20style-black-000000.svg">
  </a>
  <a href="LICENSE">
    <img alt="License: MIT" src="https://img.shields.io/badge/License-MIT-blue.svg">
  </a>
</p>

---

## ğŸ“š BibliografÃ­a base (The Canon)

Las referencias bibliogrÃ¡ficas del programa se apoyan principalmente en:

- **[Wasserman]** â€” *All of Statistics: A Concise Course in Statistical Inference* (Larry Wasserman)
- **[ESL]** â€” *The Elements of Statistical Learning* (Hastie, Tibshirani & Friedman)
- **[BDA3]** â€” *Bayesian Data Analysis (3rd Ed.)* (Gelman, Carlin, et al.)
- **[Efron]** â€” *Computer Age Statistical Inference* (Efron & Hastie)
- **[Pearl]** â€” *Causality: Models, Reasoning, and Inference* (Judea Pearl)
- **[Shumway]** â€” *Time Series Analysis and Its Applications* (Shumway & Stoffer)

---

## ğŸ—ºï¸ Syllabus detallado

### MÃ“DULO I â€” Probabilidad multivariante y geometrÃ­a
*Prerrequisito: dominio de variables aleatorias univariadas y distribuciones estÃ¡ndar.*

1. **Vectores aleatorios y geometrÃ­a de datos**
   - NotaciÃ³n matricial: vector de medias \(\mu\) y matriz de covarianza \(\Sigma\).
   - Dependencia lineal vs. independencia estadÃ­stica.
   - **BibliografÃ­a:** **[Wasserman, Ch. 3]**, **[ESL, Ch. 2.4â€“2.5]**.

2. **Normal multivariante (MVN)**
   - DefiniciÃ³n y propiedades geomÃ©tricas (elipsoides de densidad).
   - DescomposiciÃ³n espectral (eigenvalues) y blanqueo de datos (*whitening*).
   - MVN como base para Gaussian Processes y PCA.
   - **BibliografÃ­a:** **[ESL, Ch. 4.3]**, **[Wasserman, Ch. 14]**.

3. **TeorÃ­a del aprendizaje (concentraciÃ³n de la medida)**
   - RevisiÃ³n de LLN y CLT desde la convergencia estocÃ¡stica.
   - Desigualdades de **Chebyshev** y **Hoeffding**: Â¿por quÃ© aprenden las mÃ¡quinas? (cotas de error).
   - **BibliografÃ­a:** **[Wasserman, Ch. 4â€“5]**.

---

### MÃ“DULO II â€” Inferencia computacional y â€œmodernaâ€
*Enfoque: superar las limitaciones de las pruebas de hipÃ³tesis â€œde tablaâ€ mediante simulaciÃ³n.*

4. **TeorÃ­a asintÃ³tica y mÃ¡xima verosimilitud (MLE)**
   - MLE como problema de optimizaciÃ³n.
   - Propiedades: consistencia, eficiencia y normalidad asintÃ³tica.
   - InformaciÃ³n de Fisher y cota de CramÃ©râ€“Rao (lÃ­mite de precisiÃ³n).
   - **BibliografÃ­a:** **[Wasserman, Ch. 9]**.

5. **Bootstrap y mÃ©todos de resampling**
   - EstimaciÃ³n del error estÃ¡ndar sin fÃ³rmulas cerradas.
   - Bootstrap paramÃ©trico vs. no paramÃ©trico. Intervalos de confianza BCa.
   - **BibliografÃ­a:** **[Wasserman, Ch. 8]**, **[Efron, Ch. 10â€“11]**.

6. **Tests de hipÃ³tesis en alta dimensionalidad**
   - Tests de Wald, Score y Likelihood Ratio Test (LRT).
   - Comparaciones mÃºltiples y riesgo de *p-hacking*.
   - Bonferroni y False Discovery Rate (FDR â€” Benjaminiâ€“Hochberg).
   - **BibliografÃ­a:** **[Wasserman, Ch. 10]**, **[Efron, Ch. 15]**.

---

### MÃ“DULO III â€” Aprendizaje estadÃ­stico (regresiÃ³n avanzada)
*Enfoque: trade-off sesgoâ€“varianza y selecciÃ³n de modelos.*

7. **GeometrÃ­a de mÃ­nimos cuadrados (OLS)**
   - RegresiÃ³n como proyecciÃ³n ortogonal en subespacios lineales.
   - Teorema de Gaussâ€“Markov.
   - DiagnÃ³stico: leverage, distancia de Cook y anÃ¡lisis de residuos.
   - **BibliografÃ­a:** **[ESL, Ch. 3.2]**, **[Wasserman, Ch. 13]**.

8. **RegularizaciÃ³n y selecciÃ³n de modelos**
   - MaldiciÃ³n de la dimensionalidad (\(p > n\)).
   - **Ridge (L2):** contracciÃ³n de coeficientes y priors gaussianos.
   - **Lasso (L1):** sparsity y selecciÃ³n de variables (priors de Laplace).
   - Criterios de informaciÃ³n: AIC, BIC y Mallowsâ€™ \(C_p\).
   - **BibliografÃ­a:** **[ESL, Ch. 3.4 y Ch. 7]**.

9. **Modelos lineales generalizados (GLM)**
   - Familia exponencial de distribuciones.
   - FunciÃ³n de enlace (*link*): logÃ­stica (clasificaciÃ³n) y Poisson (conteos).
   - IRLS (Iteratively Reweighted Least Squares).
   - **BibliografÃ­a:** **[ESL, Ch. 4.4]**, **[McCullagh & Nelder / Wasserman, Ch. 13]**.

---

### MÃ“DULO IV â€” Modelos estructurados, grafos y tiempo
*Enfoque: modelar dependencias complejas y causalidad.*

10. **Probabilistic Graphical Models (PGMs)**
   - **DAGs:** grafos acÃ­clicos dirigidos y factorizaciÃ³n de la conjunta.
   - Independencia condicional y **d-separation** (lectura de grafos).
   - *Plate notation* para modelos jerÃ¡rquicos.
   - **BibliografÃ­a:** **[Wasserman, Ch. 17]**, **[Bishop, Ch. 8]**.

11. **Inferencia causal**
   - CorrelaciÃ³n vs. causaciÃ³n: la escalera de Pearl.
   - Intervenciones: operador \(do(x)\).
   - Confounders, colliders y criterio *back-door*.
   - **BibliografÃ­a:** **[Pearl, Ch. 1â€“3]**, **[Wasserman, Ch. 19]**.

12. **Inferencia bayesiana y MCMC**
   - Priors conjugados vs. no informativos.
   - La posterior como distribuciÃ³n de creencias.
   - IntroducciÃ³n a Markov Chain Monte Carlo (Metropolisâ€“Hastings).
   - **BibliografÃ­a:** **[BDA3, Ch. 1â€“3]**, **[Wasserman, Ch. 11]**.

13. **Series temporales (grafos dinÃ¡micos)**
   - Procesos estocÃ¡sticos: estacionariedad y autocorrelaciÃ³n.
   - Modelos ARIMA.
   - Modelos de espacio de estados (SSM) y filtro de Kalman.
   - **BibliografÃ­a:** **[Shumway, Ch. 1â€“3 y Ch. 6]**.

---

## ğŸ§± Estructura del repositorio (high-level)

> *AjustÃ¡ estos nombres si difieren en tu repo.*

- `docs/` â€” sitio MkDocs / GitHub Pages (material por mÃ³dulo)
- `src/classroom/` â€” notebooks, scripts y prÃ¡cticas por mÃ³dulo
- `assets/` â€” imÃ¡genes, gifs y recursos visuales del curso
- `tests/` â€” tests (si aplica)
- `requirements.txt` / `pyproject.toml` â€” dependencias

---

## ğŸ› ï¸ Uso y ejecuciÃ³n

### 1) Clonar el repo

```bash
git clone https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos.git
cd estadisticas-para-ciencia-de-datos
2) Crear entorno e instalar dependencias (Python)
python -m venv .venv
# Linux/macOS:
source .venv/bin/activate
# Windows:
# .venv\Scripts\activate

pip install -U pip
pip install -r requirements.txt
3) Calidad de cÃ³digo (opcional, recomendado)
pre-commit install
pre-commit run --all-files
4) Ejecutar notebooks / prÃ¡cticas
Si trabajÃ¡s con Jupyter:

jupyter lab
Si usÃ¡s VSCode: abrÃ­ la carpeta del repo y ejecutÃ¡ notebooks desde la extensiÃ³n de Jupyter.

âœ… Requisitos y nivel esperado
Probabilidad y estadÃ­stica clÃ¡sica (intervalos, tests bÃ¡sicos, ANOVA)

Ãlgebra lineal (vectores, matrices, autovalores/autovectores)

ProgramaciÃ³n: Python (recomendado); R/Julia como soporte (opcional)

ğŸ¤ Contribuciones
Issues y PRs son bienvenidos. Si vas a proponer cambios grandes (estructura, syllabus o tooling), abrÃ­ primero un issue describiendo el objetivo y el impacto.

âš–ï¸ Licencia
Este material se distribuye bajo licencia MIT. Ver el archivo LICENSE.

ğŸ“Œ Referencias (citas cortas)
[Wasserman] Larry Wasserman â€” All of Statistics

[ESL] Hastie, Tibshirani, Friedman â€” The Elements of Statistical Learning

[BDA3] Gelman et al. â€” Bayesian Data Analysis (3rd Ed.)

[Efron] Efron & Hastie â€” Computer Age Statistical Inference

[Pearl] Judea Pearl â€” Causality

[Shumway] Shumway & Stoffer â€” Time Series Analysis and Its Applications
