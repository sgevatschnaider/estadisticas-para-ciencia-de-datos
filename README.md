# Applied Statistics for Data Science (Engineering & Research Track)

<p align="center">
  <img src="assets/portada%20.gif" alt="VisualizaciÃ³n de conceptos: GeometrÃ­a de Datos, Inferencia, Learning, Modelos Estructurados y Grafos" width="100%">
  <br>
  <sub style="font-size: 14px;">
    ğŸ”µ <b>GeometrÃ­a & MVN</b> (MÃ³d. I) &nbsp;|&nbsp;
    ğŸŸ¢ <b>Inferencia Comp.</b> (MÃ³d. II) &nbsp;|&nbsp;
    ğŸŸ  <b>RegularizaciÃ³n & ML</b> (MÃ³d. III)
    <br>
    ğŸ”´ <b>Causalidad & Tiempo</b> (MÃ³d. IV) &nbsp;|&nbsp;
    ğŸŸ£ <b>Grafos & Complejidad</b> (MÃ³d. V)
  </sub>
</p>

---

## ğŸ“‘ Tabla de Contenidos
1. [Accesos rÃ¡pidos](#-accesos-rÃ¡pidos)
2. [DescripciÃ³n](#-descripciÃ³n)
3. [BibliografÃ­a](#-bibliografÃ­a-base-the-canon)
4. [Syllabus](#-syllabus-detallado)
5. [InstalaciÃ³n y Uso](#-uso-y-ejecuciÃ³n)

---

## ğŸ”— Accesos rÃ¡pidos

### âœ… PÃ¡ginas del curso (DocumentaciÃ³n Web)
> *Acceso a la teorÃ­a y notas de clase renderizadas en HTML.*

### âœ… PÃ¡ginas del curso (DocumentaciÃ³n Web)
> *Acceso a la teorÃ­a y notas de clase renderizadas en HTML.*

[![MÃ“DULO I â€” GeometrÃ­a](https://img.shields.io/badge/M%C3%93DULO%20I-geometrÃ­a-0366d6)](https://sgevatschnaider.github.io/estadisticas-para-ciencia-de-datos/docs/modulos/probabilidad.html)
[![MÃ“DULO II â€” Inferencia](https://img.shields.io/badge/M%C3%93DULO%20II-inferencia-0366d6)](https://sgevatschnaider.github.io/estadisticas-para-ciencia-de-datos/docs/modulos/inferencia.html)
[![MÃ“DULO III â€” Learning](https://img.shields.io/badge/M%C3%93DULO%20III-learning-0366d6)](https://sgevatschnaider.github.io/estadisticas-para-ciencia-de-datos/docs/modulos/learning.html)
[![MÃ“DULO IV â€” Estructurados](https://img.shields.io/badge/M%C3%93DULO%20IV-estructurados-0366d6)](https://sgevatschnaider.github.io/estadisticas-para-ciencia-de-datos/docs/modulos/pgm.html)
[![MÃ“DULO V â€” Grafos](https://img.shields.io/badge/M%C3%93DULO%20V-grafos-0366d6)](https://sgevatschnaider.github.io/estadisticas-para-ciencia-de-datos/docs/modulos/tiempo.html)

### ğŸ§© CÃ³digo del curso (Notebooks y Scripts)
> *Acceso directo al cÃ³digo fuente en el repositorio.*

[![src/classroom/geometria](https://img.shields.io/badge/src-geometria-0366d6)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/probabilidad)
[![src/classroom/inferencia](https://img.shields.io/badge/src-inferencia-0366d6)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/inferencia)
[![src/classroom/learning](https://img.shields.io/badge/src-learning-0366d6)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/learning)
[![src/classroom/estructurados](https://img.shields.io/badge/src-estructurados-0366d6)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/pgm)
[![src/classroom/grafos](https://img.shields.io/badge/src-grafos-0366d6)](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/tiempo)

<p align="center">
  <a href="https://sgevatschnaider.github.io/estadisticas-para-ciencia-de-datos/" target="_blank" rel="noopener">
    <img alt="Live docs â€” GitHub Pages" src="https://img.shields.io/badge/Live%20docs-GitHub%20Pages-2b3137?style=for-the-badge&logo=github" />
  </a>
  &nbsp;
  <a href="#" target="_blank" rel="noopener">
    <img alt="Live demos â€” Binder" src="https://img.shields.io/badge/Live%20demos-Binder-f5a250?style=for-the-badge&logo=jupyter" />
  </a>
</p>

---



## ğŸ¯ DescripciÃ³n

Material docente de nivel **IngenierÃ­a** para Ciencia de Datos.

El curso asume conocimientos previos de estadÃ­stica clÃ¡sica (por ejemplo: ANOVA y tests de hipÃ³tesis bÃ¡sicos) y se centra en la **â€œcaja blancaâ€** de la ingenierÃ­a estadÃ­stica moderna: inferencia computacional, geometrÃ­a de los datos, regularizaciÃ³n, y modelos grÃ¡ficos probabilÃ­sticos (PGMs), con extensiÃ³n a causalidad y series temporales.

<p align="center">
  <a href="https://www.python.org/">
    <img alt="Python" src="https://img.shields.io/badge/python-3.10%2B-blue">
  </a>
  <a href="https://www.r-project.org/">
    <img alt="R" src="https://img.shields.io/badge/R-4.x-276DC3">
  </a>
  <a href="https://julialang.org/">
    <img alt="Julia" src="https://img.shields.io/badge/Julia-1.x-9558B2">
  </a>
  <a href="https://github.com/psf/black">
    <img alt="Code style: black" src="https://img.shields.io/badge/code%20style-black-000000.svg">
  </a>
  <a href="LICENSE">
    <img alt="License: MIT" src="https://img.shields.io/badge/License-MIT-blue.svg">
  </a>
</p>

---

## ğŸ“š BibliografÃ­a base (The Canon)

Las referencias bibliogrÃ¡ficas del programa se apoyan principalmente en:

- **[Wasserman]** â€” *All of Statistics: A Concise Course in Statistical Inference* (Larry Wasserman)
- **[ESL]** â€” *The Elements of Statistical Learning* (Hastie, Tibshirani & Friedman)
- **[BDA3]** â€” *Bayesian Data Analysis (3rd Ed.)* (Gelman, Carlin, et al.)
- **[Efron]** â€” *Computer Age Statistical Inference* (Efron & Hastie)
- **[Pearl]** â€” *Causality: Models, Reasoning, and Inference* (Judea Pearl)
- **[Shumway]** â€” *Time Series Analysis and Its Applications* (Shumway & Stoffer)

---

## ğŸ¯ PropÃ³sito del curso

Conectar **probabilidad multivariante + Ã¡lgebra lineal + inferencia computacional** con la **geometrÃ­a de los datos** usada hoy en **ML/IA**: reducciÃ³n de dimensiÃ³n, regularizaciÃ³n, modelos estructurados, grafos y mÃ©todos de simulaciÃ³n/optimizaciÃ³n.

---

## ğŸ—ºï¸ Syllabus detallado

### MÃ“DULO I â€” Vectores aleatorios y geometrÃ­a de datos

*Enfoque: la distribuciÃ³n conjunta como objeto geomÃ©trico (subespacios, elipsoides, proyecciones, mÃ©tricas).*

1. **NotaciÃ³n matricial: vector de medias  y matriz de covarianza**
* DefiniciÃ³n de  y .
* Propiedades: simetrÃ­a, semidefinida positiva, interpretaciÃ³n geomÃ©trica (varianza por direcciones).
* Varianza de proyecciones:  y mÃ©trica inducida.
* **AplicaciÃ³n en DS/ML:** Feature scaling, *Covariance shift*, distancia de Mahalanobis (detecciÃ³n de outliers).
* **BibliografÃ­a:** **[Wasserman, Ch. 3]**, **[ESL, Ch. 2.4â€“2.5]**.


2. **Dependencia lineal vs. independencia estadÃ­stica**
* Dependencia lineal ( rango, singularidad) vs. Independencia estadÃ­stica ().
* CorrelaciÃ³n  independencia (salvo en Gaussianas).
* **AplicaciÃ³n en DS/ML:** Multicolinealidad en regresiÃ³n (Ridge/Lasso), *Feature redundancy*, correlaciÃ³n vs. causalidad.
* **BibliografÃ­a:** **[Koller & Friedman, Ch. 2â€“3]**, **[Bishop, Ch. 8]**, **[Wasserman, Ch. 3â€“4]**.


3. **Normal multivariante (MVN) y geometrÃ­a**
* DefiniciÃ³n : forma cuadrÃ¡tica, log-densidad y energÃ­a.
* **Elipsoides de densidad:** InterpretaciÃ³n geomÃ©trica y descomposiciÃ³n espectral ().
* **Whitening:** Blanqueo de datos .
* **AplicaciÃ³n en DS/ML:** LDA/QDA, PCA (rotaciÃ³n + escalado), Batch/Layer Normalization, Gaussian Processes.
* **BibliografÃ­a:** **[Wasserman, Ch. 14]**, **[Mardiaâ€“Kentâ€“Bibby]**, **[ESL, Ch. 3.5 / 14]**, **[Bishop, PCA]**.


4. **TeorÃ­a del aprendizaje: concentraciÃ³n de la medida**
* Convergencia en probabilidad vs. en distribuciÃ³n (LLN y CLT).
* Desigualdades de **Chebyshev** y **Hoeffding**: cotas no asintÃ³ticas del error.
* **AplicaciÃ³n en DS/ML:** GeneralizaciÃ³n (riesgo empÃ­rico vs. poblacional), intervalos de confianza para mÃ©tricas (Accuracy, AUC).
* **BibliografÃ­a:** **[Wasserman, Ch. 4â€“5]**.



---

### MÃ“DULO II â€” Inferencia computacional y â€œmodernaâ€

*Enfoque: superar las limitaciones de las pruebas de hipÃ³tesis clÃ¡sicas mediante optimizaciÃ³n y simulaciÃ³n.*

5. **TeorÃ­a asintÃ³tica y mÃ¡xima verosimilitud (MLE)**
* MLE como optimizaciÃ³n:  (Gradiente/Hessiano).
* Consistencia, eficiencia, InformaciÃ³n de Fisher y cota de CramÃ©râ€“Rao.
* **AplicaciÃ³n en DS/ML:** RegresiÃ³n logÃ­stica (Cross-entropy), entrenamiento de modelos probabilÃ­sticos (Naive Bayes, HMM).
* **BibliografÃ­a:** **[Wasserman, Ch. 9]**, **[Efron & Hastie, Ch. 2]**.


6. **Bootstrap y mÃ©todos de resampling**
* EstimaciÃ³n del error estÃ¡ndar sin fÃ³rmulas cerradas.
* Bootstrap paramÃ©trico vs. no paramÃ©trico. Intervalos **BCa**.
* **AplicaciÃ³n en DS/ML:** Incertidumbre en mÃ©tricas (F1, AUC), Bagging (Random Forest), *Stability selection*.
* **BibliografÃ­a:** **[Wasserman, Ch. 8]**, **[Efron & Hastie, Ch. 10â€“11]**.


7. **Tests de hipÃ³tesis en alta dimensionalidad**
* Tests de Wald, Score y Likelihood Ratio Test (LRT).
* Comparaciones mÃºltiples: Bonferroni y **False Discovery Rate (FDR â€” Benjaminiâ€“Hochberg)**.
* **AplicaciÃ³n en DS/ML:** A/B testing a escala, selecciÃ³n de features en alta dimensiÃ³n (genes, texto).
* **BibliografÃ­a:** **[Wasserman, Ch. 10]**, **[ESL, High-Dim Problems]**, **[Efron & Hastie, Ch. 15]**.



---

### MÃ“DULO III â€” Aprendizaje estadÃ­stico (regresiÃ³n avanzada)

*Enfoque: trade-off sesgoâ€“varianza, geometrÃ­a de proyecciones y selecciÃ³n de modelos.*

8. **GeometrÃ­a de mÃ­nimos cuadrados (OLS)**
* RegresiÃ³n como proyecciÃ³n ortogonal: Matriz sombrero .
* Teorema de Gaussâ€“Markov y diagnÃ³stico (leverage, distancia de Cook).
* **AplicaciÃ³n en DS/ML:** Baselines interpretables, detecciÃ³n de puntos influyentes en producciÃ³n.
* **BibliografÃ­a:** **[ESL, Ch. 3.2]**, **[Wasserman, Ch. 13]**.


9. **RegularizaciÃ³n y selecciÃ³n de modelos**
* MaldiciÃ³n de la dimensionalidad ().
* **Ridge (L2):** ContracciÃ³n y priors gaussianos.
* **Lasso (L1):** Sparsity y priors de Laplace. Elastic Net.
* Criterios de informaciÃ³n: AIC, BIC, .
* **AplicaciÃ³n en DS/ML:** Estabilidad en embeddings, selecciÃ³n automÃ¡tica de variables, *Early stopping*.
* **BibliografÃ­a:** **[ESL, Ch. 3.4 y Ch. 7]**, **[Efron & Hastie, Ch. 7 y 16]**.


10. **Modelos lineales generalizados (GLM)**
* Familia exponencial y funciones de enlace (*link*): LogÃ­stica y Poisson.
* Algoritmo IRLS (Iteratively Reweighted Least Squares).
* **AplicaciÃ³n en DS/ML:** ClasificaciÃ³n calibrada, modelado de conteos/demanda.
* **BibliografÃ­a:** **[ESL, Ch. 4.4]**, **[Efron & Hastie, Ch. 8]**.



---

### MÃ“DULO IV â€” Modelos estructurados, grafos, causalidad y tiempo

*Enfoque: modelar dependencias complejas, inferencia bayesiana y dinÃ¡mica.*

11. **Probabilistic Graphical Models (PGMs)**
* **DAGs:** FactorizaciÃ³n de la conjunta, independencia condicional y **d-separation**.
* *Plate notation* para modelos jerÃ¡rquicos.
* **AplicaciÃ³n en DS/ML:** Naive Bayes, HMMs, modelos jerÃ¡rquicos multitenant.
* **BibliografÃ­a:** **[Koller & Friedman, Ch. 2â€“4]**, **[Bishop, Ch. 8]**.


12. **Inferencia causal**
* CorrelaciÃ³n vs. CausaciÃ³n. Intervenciones y operador .
* Confounders, colliders y criterio *back-door*.
* **AplicaciÃ³n en DS/ML:** A/B testing, Uplift modeling, eliminaciÃ³n de sesgos en datos.
* **BibliografÃ­a:** **[Pearl, Ch. 1â€“3]**, **[Wasserman, Ch. 16â€“17]**.


13. **Inferencia bayesiana y MCMC**
* Priors conjugados vs. no informativos. Posterior como creencia.
* **MCMC:** Metropolisâ€“Hastings y diagnÃ³stico bÃ¡sico.
* **AplicaciÃ³n en DS/ML:** Bayesian Logistic Regression, Bayesian Optimization, cuantificaciÃ³n de incertidumbre.
* **BibliografÃ­a:** **[BDA3, Ch. 1â€“3]**, **[Efron & Hastie, Ch. 13]**.


14. **Series temporales y modelos dinÃ¡micos**
* Estacionariedad, autocorrelaciÃ³n y modelos ARIMA.
* **State Space Models (SSM):** Filtro de Kalman.
* **AplicaciÃ³n en DS/ML:** Forecasting (demanda, finanzas), tracking (IoT), comparaciÃ³n con RNNs.
* **BibliografÃ­a:** **[Shumway & Stoffer, Ch. 1â€“3 y 6]**, **[Bishop, Ch. 13]**.



---

### MÃ“DULO V â€” GeometrÃ­a moderna: grafos, complejidad y bÃºsqueda

*Enfoque: expandir la geometrÃ­a mÃ¡s allÃ¡ de lo euclÃ­deo (grafos), dinÃ¡mica local (autÃ³matas) y optimizaciÃ³n no convexa.*

15. **TeorÃ­a de grafos para ciencia de datos**
* Matrices de adyacencia, conectividad, caminos, BFS/DFS.
* **AplicaciÃ³n en DS/ML:** Network science (PageRank, comunidades), recomendadores, detecciÃ³n de fraude.
* **BibliografÃ­a:** **[Benjaminâ€“Chartrandâ€“Zhang]**, **[Kumar]**.


16. **Espectros de grafos y geometrÃ­a (Spectral Graph Theory)**
* Espectro del Laplaciano; autovalores y autovectores en grafos.
* **AplicaciÃ³n en DS/ML:** Spectral clustering, Graph Embeddings, fundamentos de GNNs.
* **BibliografÃ­a:** **[Kumar]**, **[ESL/Bishop (ref. espectral)]**.


17. **Modelos sobre grafos y unificaciÃ³n**
* Markov blankets, MRF (Markon Random Fields) y Factor Graphs.
* Inferencia aproximada (Loopy Belief Propagation).
* **AplicaciÃ³n en DS/ML:** CRFs (secuencias), segmentaciÃ³n de imÃ¡genes, denoising.
* **BibliografÃ­a:** **[Koller & Friedman]**, **[Bishop, Ch. 8]**.


18. **Algoritmos evolutivos y programaciÃ³n genÃ©tica (GP)**
* OptimizaciÃ³n sin gradiente en paisajes no convexos.
* GP: BÃºsqueda en espacios de programas (*Symbolic Regression*).
* **AplicaciÃ³n en DS/ML:** AutoML, descubrimiento de ecuaciones interpretables, Neuroevolution.
* **BibliografÃ­a:** **[Material de CÃ¡tedra: Evolutivos/GP]**.


19. **AutÃ³matas celulares (CA) y dinÃ¡mica local**
* Reglas locales â†’ Emergencia global. DinÃ¡mica en grillas.
* **AplicaciÃ³n en DS/ML:** SimulaciÃ³n de propagaciÃ³n (epidemias, trÃ¡fico), modelos generativos discretos.
* **BibliografÃ­a:** **[Material de CÃ¡tedra: AutÃ³matas Celulares]**, **[Wolfram (ref. conceptual)]**.



---

## ğŸ“š BibliografÃ­a General del Curso

* **Wasserman** â€” *All of Statistics*.
* **Hastie, Tibshirani, Friedman** â€” *The Elements of Statistical Learning (ESL)*.
* **Efron & Hastie** â€” *Computer Age Statistical Inference (CASI)*.
* **Bishop** â€” *Pattern Recognition and Machine Learning (PRML)*.
* **Koller & Friedman** â€” *Probabilistic Graphical Models*.
* **Gelman et al.** â€” *Bayesian Data Analysis (BDA3)*.
* **Pearl** â€” *Causal Inference in Statistics*.
* **Shumway & Stoffer** â€” *Time Series Analysis and Its Applications*.
* **Mardia, Kent, Bibby** â€” *Multivariate Analysis*.
* **Benjamin, Chartrand, Zhang** â€” *The Fascinating World of Graph Theory*.

## ğŸ§± Estructura del repositorio (high-level)

> *AjustÃ¡ estos nombres si difieren en tu repo.*

- `docs/` â€” sitio MkDocs / GitHub Pages (material por mÃ³dulo)
- `src/classroom/` â€” notebooks, scripts y prÃ¡cticas por mÃ³dulo
- `assets/` â€” imÃ¡genes, gifs y recursos visuales del curso
- `tests/` â€” tests (si aplica)
- `requirements.txt` / `pyproject.toml` â€” dependencias

---

## ğŸ› ï¸ Uso y ejecuciÃ³n

### 1) Clonar el repositorio

```bash
git clone [https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos.git](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos.git)
cd estadisticas-para-ciencia-de-datos

```

### 2) Crear entorno e instalar dependencias (Python)

Se recomienda usar un entorno virtual para evitar conflictos.

```bash
# Crear entorno virtual
python -m venv .venv

# Activar entorno (Linux/macOS):
source .venv/bin/activate

# Activar entorno (Windows):
# .venv\Scripts\activate

# Instalar dependencias
pip install -U pip
pip install -r requirements.txt

```

### 3) Calidad de cÃ³digo (opcional, recomendado)

```bash
pre-commit install
pre-commit run --all-files

```

### 4) Ejecutar notebooks / prÃ¡cticas

Si trabajÃ¡s con **Jupyter Lab**:

```bash
jupyter lab

```

Si usÃ¡s **VSCode**:

> Abre la carpeta del repositorio y ejecuta los archivos `.ipynb` directamente usando la extensiÃ³n de Jupyter.

---

## âœ… Requisitos y nivel esperado

* **MatemÃ¡tica:**
* Probabilidad y estadÃ­stica clÃ¡sica (intervalos, tests bÃ¡sicos, ANOVA).
* Ãlgebra lineal (vectores, matrices, autovalores/autovectores).


* **ProgramaciÃ³n:**
* **Python** (lenguaje principal).
* *R / Julia* (como soporte opcional).



## ğŸ¤ Contribuciones

Issues y Pull Requests son bienvenidos.

* Si vas a proponer cambios grandes (estructura, syllabus o tooling), por favor abrÃ­ primero un **Issue** describiendo el objetivo y el impacto.

## âš–ï¸ Licencia

Este material se distribuye bajo licencia **MIT**. Ver el archivo [LICENSE](https://www.google.com/search?q=LICENSE) para mÃ¡s detalles.

## ğŸ“Œ Referencias (Citas cortas)

* **[Wasserman]** Larry Wasserman â€” *All of Statistics*
* **[ESL]** Hastie, Tibshirani, Friedman â€” *The Elements of Statistical Learning*
* **[BDA3]** Gelman et al. â€” *Bayesian Data Analysis (3rd Ed.)*
* **[Efron]** Efron & Hastie â€” *Computer Age Statistical Inference*
* **[Pearl]** Judea Pearl â€” *Causality*
* **[Shumway]** Shumway & Stoffer â€” *Time Series Analysis and Its Applications*

