Applied Statistics for Data Science (Engineering & Research Track)

<p align="center">
  <img src="assets/portada%20.gif" alt="Visualizaci√≥n de conceptos: Geometr√≠a de Datos, Inferencia, Learning, Modelos Estructurados y Grafos" width="100%">
  <br>
  <sub style="font-size: 14px;">
    üîµ <b>Geometr√≠a & MVN</b> (M√≥d. I) &nbsp;|&nbsp;
    üü¢ <b>Inferencia Comp.</b> (M√≥d. II) &nbsp;|&nbsp;
    üü† <b>Regularizaci√≥n & ML</b> (M√≥d. III)
    <br>
    üî¥ <b>Causalidad & Tiempo</b> (M√≥d. IV) &nbsp;|&nbsp;
    üü£ <b>Grafos & Complejidad</b> (M√≥d. V)
  </sub>
</p>

---

## üìë Tabla de Contenidos
1. [Accesos r√°pidos](#accesos-r√°pidos)
2. [Descripci√≥n](#descripci√≥n)
3. [Bibliograf√≠a Base](#bibliograf√≠a-base)
4. [Syllabus Detallado](#syllabus-detallado)
5. [Instalaci√≥n y Uso](#c√≥mo-utilizar-este-material-instalaci√≥n-y-uso)
   
## üîó Accesos r√°pidos

> *Acceso directo a las carpetas del repositorio con la teor√≠a (simuladores HTML) y el c√≥digo fuente (Notebooks).*

<p align="center">
  <a href="https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/probabilidad"><img src="https://img.shields.io/badge/M√ìDULO_I-Geometr√≠a-0366d6?style=for-the-badge"></a>
  <a href="https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/inferencia"><img src="https://img.shields.io/badge/M√ìDULO_II-Inferencia-0366d6?style=for-the-badge"></a>
  <a href="https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/learning"><img src="https://img.shields.io/badge/M√ìDULO_III-Learning-0366d6?style=for-the-badge"></a>
  <a href="https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/pgm"><img src="https://img.shields.io/badge/M√ìDULO_IV-Estructurados-0366d6?style=for-the-badge"></a>
  <a href="https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/tiempo"><img src="https://img.shields.io/badge/M√ìDULO_V-Grafos-0366d6?style=for-the-badge"></a>
<p align="center">
  <a href="https://sgevatschnaider.github.io/estadisticas-para-ciencia-de-datos/" target="_blank" rel="noopener">
    <img alt="Live docs ‚Äî GitHub Pages" src="https://img.shields.io/badge/Live%20docs-GitHub%20Pages-2b3137?style=for-the-badge&logo=github" />
  </a>
  &nbsp;
  <a href="https://mybinder.org/v2/gh/sgevatschnaider/estadisticas-para-ciencia-de-datos/main" target="_blank" rel="noopener">
    <img alt="Live demos ‚Äî Binder" src="https://img.shields.io/badge/Live%20demos-Binder-f5a250?style=for-the-badge&logo=jupyter" />
  </a>
  &nbsp;
  <a href="https://colab.research.google.com/github/sgevatschnaider/estadisticas-para-ciencia-de-datos/" target="_blank" rel="noopener">
    <img alt="Open in ‚Äî Colab" src="https://img.shields.io/badge/Open%20in-Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white" />
  </a>
</p>

## üéØ Descripci√≥n

Material docente de nivel **Ingenier√≠a** para Ciencia de Datos.

El curso asume conocimientos previos de estad√≠stica cl√°sica (por ejemplo: ANOVA y tests de hip√≥tesis b√°sicos) y se centra en la **‚Äúcaja blanca‚Äù** de la ingenier√≠a estad√≠stica moderna: inferencia computacional, geometr√≠a de los datos, regularizaci√≥n, y modelos gr√°ficos probabil√≠sticos (PGMs), con extensi√≥n a causalidad y series temporales.

<p align="center">
  <a href="https://www.python.org/">
    <img alt="Python" src="https://img.shields.io/badge/python-3.10%2B-blue">
  </a>
  <a href="https://www.r-project.org/">
    <img alt="R" src="https://img.shields.io/badge/R-4.x-276DC3">
  </a>
  <a href="https://julialang.org/">
    <img alt="Julia" src="https://img.shields.io/badge/Julia-1.x-9558B2">
  </a>
  <a href="https://github.com/psf/black">
    <img alt="Code style: black" src="https://img.shields.io/badge/code%20style-black-000000.svg">
  </a>
  <a href="LICENSE">
    <img alt="License: MIT" src="https://img.shields.io/badge/License-MIT-blue.svg">
  </a>
</p>

---

## üìö Bibliograf√≠a base (The Canon)

Las referencias bibliogr√°ficas del programa se apoyan principalmente en:

- **[Wasserman]** ‚Äî *All of Statistics: A Concise Course in Statistical Inference* (Larry Wasserman)
- **[ESL]** ‚Äî *The Elements of Statistical Learning* (Hastie, Tibshirani & Friedman)
- **[BDA3]** ‚Äî *Bayesian Data Analysis (3rd Ed.)* (Gelman, Carlin, et al.)
- **[Efron]** ‚Äî *Computer Age Statistical Inference* (Efron & Hastie)
- **[Pearl]** ‚Äî *Causality: Models, Reasoning, and Inference* (Judea Pearl)
- **[Shumway]** ‚Äî *Time Series Analysis and Its Applications* (Shumway & Stoffer)

---

## üéØ Prop√≥sito del curso

Conectar **probabilidad multivariante + √°lgebra lineal + inferencia computacional** con la **geometr√≠a de los datos** usada hoy en **ML/IA**: reducci√≥n de dimensi√≥n, regularizaci√≥n, modelos estructurados, grafos y m√©todos de simulaci√≥n/optimizaci√≥n.

---

## üó∫Ô∏è Syllabus detallado

# M√ìDULO I ‚Äî Vectores Aleatorios y Geometr√≠a de Datos

> **Enfoque:** La distribuci√≥n conjunta como objeto geom√©trico (subespacios, elipsoides, proyecciones) y la teor√≠a de concentraci√≥n de la medida.

---

## üìÇ Ruta del m√≥dulo y Recursos

| Recurso | Enlace al Repositorio |
| :--- | :--- |
| **üíª C√≥digo Fuente** | [Ir a carpeta `src/classroom/probabilidad`](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/probabilidad) |
| **üìù Ejercicios** | [Ver ejercicios](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/probabilidad/ejercicios) |
| **üí° Soluciones** | [Ver soluciones](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos/tree/main/src/classroom/probabilidad/soluciones) |

---

## üó∫Ô∏è Temario Detallado

### 1. Notaci√≥n matricial y Geometr√≠a ($\mu, \Sigma$)
* **Definiciones:** Vector de medias $\mu = \mathbb{E}[X]$ y matriz de covarianza $\Sigma = \mathbb{E}[(X-\mu)(X-\mu)^\top]$.
* **Propiedades:** Simetr√≠a, semidefinida positiva e interpretaci√≥n geom√©trica (varianza por direcciones).
* **Proyecciones:** Varianza de $u^\top X$ y m√©trica inducida.
* *Aplicaci√≥n:* Feature scaling, Covariance shift, Distancia de Mahalanobis.

### 2. Dependencia Lineal vs. Independencia Estad√≠stica
* **Conceptos:** Diferencia entre dependencia lineal (rango, singularidad) e independencia estad√≠stica ($p(x,y)=p(x)p(y)$).
* **La falacia de la correlaci√≥n:** Correlaci√≥n $\neq$ Independencia (excepto en Gaussianas).
* *Aplicaci√≥n:* Multicolinealidad en regresi√≥n, selecci√≥n de variables.

### 3. Normal Multivariante (MVN)
* **Definici√≥n:** Densidad, forma cuadr√°tica y energ√≠a.
* **Geometr√≠a:** Elipsoides de densidad y descomposici√≥n espectral ($\Sigma = U \Lambda U^\top$).
* **Whitening (Blanqueo):** Transformaci√≥n $Z=\Lambda^{-1/2}U^\top(X-\mu)$ para decorrelacionar datos.
* *Aplicaci√≥n:* PCA, Gaussian Processes, Batch Normalization.

### 4. Teor√≠a del Aprendizaje (Concentraci√≥n de la Medida)
* **Convergencia:** Ley de los Grandes N√∫meros (LLN) y Teorema Central del L√≠mite (CLT) vistos como convergencia estoc√°stica.
* **Desigualdades:**
    * **Chebyshev:** Cota b√°sica basada en varianza.
    * **Hoeffding:** Cota exponencial para variables acotadas (fundamental en ML).
* *Aplicaci√≥n:* Cotas de error en generalizaci√≥n, intervalos de confianza para m√©tricas.

---

## üìö Bibliograf√≠a Espec√≠fica

* **Wasserman, *All of Statistics*:**
    * Chapter 3: Expectation & Variance.
    * Chapter 4‚Äì5: Convergence & Inequalities (Concentration).
    * Chapter 14: Multivariate Models.
* **Hastie et al., *ESL*:**
    * Chapter 2.4‚Äì2.5: Statistical Decision Theory / Local Methods.
    * Chapter 3.5: Dimensionality Reduction.
* **Koller & Friedman, *PGM*:**
    * Chapter 2‚Äì3: Independence & Factorization.

---

### M√ìDULO II ‚Äî Inferencia computacional y ‚Äúmoderna‚Äù

*Enfoque: superar las limitaciones de las pruebas de hip√≥tesis cl√°sicas mediante optimizaci√≥n y simulaci√≥n.*

5. **Teor√≠a asint√≥tica y m√°xima verosimilitud (MLE)**
* MLE como optimizaci√≥n:  (Gradiente/Hessiano).
* Consistencia, eficiencia, Informaci√≥n de Fisher y cota de Cram√©r‚ÄìRao.
* **Aplicaci√≥n en DS/ML:** Regresi√≥n log√≠stica (Cross-entropy), entrenamiento de modelos probabil√≠sticos (Naive Bayes, HMM).
* **Bibliograf√≠a:** **[Wasserman, Ch. 9]**, **[Efron & Hastie, Ch. 2]**.


6. **Bootstrap y m√©todos de resampling**
* Estimaci√≥n del error est√°ndar sin f√≥rmulas cerradas.
* Bootstrap param√©trico vs. no param√©trico. Intervalos **BCa**.
* **Aplicaci√≥n en DS/ML:** Incertidumbre en m√©tricas (F1, AUC), Bagging (Random Forest), *Stability selection*.
* **Bibliograf√≠a:** **[Wasserman, Ch. 8]**, **[Efron & Hastie, Ch. 10‚Äì11]**.


7. **Tests de hip√≥tesis en alta dimensionalidad**
* Tests de Wald, Score y Likelihood Ratio Test (LRT).
* Comparaciones m√∫ltiples: Bonferroni y **False Discovery Rate (FDR ‚Äî Benjamini‚ÄìHochberg)**.
* **Aplicaci√≥n en DS/ML:** A/B testing a escala, selecci√≥n de features en alta dimensi√≥n (genes, texto).
* **Bibliograf√≠a:** **[Wasserman, Ch. 10]**, **[ESL, High-Dim Problems]**, **[Efron & Hastie, Ch. 15]**.



---

### M√ìDULO III ‚Äî Aprendizaje estad√≠stico (regresi√≥n avanzada)

*Enfoque: trade-off sesgo‚Äìvarianza, geometr√≠a de proyecciones y selecci√≥n de modelos.*

8. **Geometr√≠a de m√≠nimos cuadrados (OLS)**
* Regresi√≥n como proyecci√≥n ortogonal: Matriz sombrero .
* Teorema de Gauss‚ÄìMarkov y diagn√≥stico (leverage, distancia de Cook).
* **Aplicaci√≥n en DS/ML:** Baselines interpretables, detecci√≥n de puntos influyentes en producci√≥n.
* **Bibliograf√≠a:** **[ESL, Ch. 3.2]**, **[Wasserman, Ch. 13]**.


9. **Regularizaci√≥n y selecci√≥n de modelos**
* Maldici√≥n de la dimensionalidad ().
* **Ridge (L2):** Contracci√≥n y priors gaussianos.
* **Lasso (L1):** Sparsity y priors de Laplace. Elastic Net.
* Criterios de informaci√≥n: AIC, BIC, .
* **Aplicaci√≥n en DS/ML:** Estabilidad en embeddings, selecci√≥n autom√°tica de variables, *Early stopping*.
* **Bibliograf√≠a:** **[ESL, Ch. 3.4 y Ch. 7]**, **[Efron & Hastie, Ch. 7 y 16]**.


10. **Modelos lineales generalizados (GLM)**
* Familia exponencial y funciones de enlace (*link*): Log√≠stica y Poisson.
* Algoritmo IRLS (Iteratively Reweighted Least Squares).
* **Aplicaci√≥n en DS/ML:** Clasificaci√≥n calibrada, modelado de conteos/demanda.
* **Bibliograf√≠a:** **[ESL, Ch. 4.4]**, **[Efron & Hastie, Ch. 8]**.



---

### M√ìDULO IV ‚Äî Modelos estructurados, grafos, causalidad y tiempo

*Enfoque: modelar dependencias complejas, inferencia bayesiana y din√°mica.*

11. **Probabilistic Graphical Models (PGMs)**
* **DAGs:** Factorizaci√≥n de la conjunta, independencia condicional y **d-separation**.
* *Plate notation* para modelos jer√°rquicos.
* **Aplicaci√≥n en DS/ML:** Naive Bayes, HMMs, modelos jer√°rquicos multitenant.
* **Bibliograf√≠a:** **[Koller & Friedman, Ch. 2‚Äì4]**, **[Bishop, Ch. 8]**.


12. **Inferencia causal**
* Correlaci√≥n vs. Causaci√≥n. Intervenciones y operador .
* Confounders, colliders y criterio *back-door*.
* **Aplicaci√≥n en DS/ML:** A/B testing, Uplift modeling, eliminaci√≥n de sesgos en datos.
* **Bibliograf√≠a:** **[Pearl, Ch. 1‚Äì3]**, **[Wasserman, Ch. 16‚Äì17]**.


13. **Inferencia bayesiana y MCMC**
* Priors conjugados vs. no informativos. Posterior como creencia.
* **MCMC:** Metropolis‚ÄìHastings y diagn√≥stico b√°sico.
* **Aplicaci√≥n en DS/ML:** Bayesian Logistic Regression, Bayesian Optimization, cuantificaci√≥n de incertidumbre.
* **Bibliograf√≠a:** **[BDA3, Ch. 1‚Äì3]**, **[Efron & Hastie, Ch. 13]**.


14. **Series temporales y modelos din√°micos**
* Estacionariedad, autocorrelaci√≥n y modelos ARIMA.
* **State Space Models (SSM):** Filtro de Kalman.
* **Aplicaci√≥n en DS/ML:** Forecasting (demanda, finanzas), tracking (IoT), comparaci√≥n con RNNs.
* **Bibliograf√≠a:** **[Shumway & Stoffer, Ch. 1‚Äì3 y 6]**, **[Bishop, Ch. 13]**.



---

### M√ìDULO V ‚Äî Geometr√≠a moderna: grafos, complejidad y b√∫squeda

*Enfoque: expandir la geometr√≠a m√°s all√° de lo eucl√≠deo (grafos), din√°mica local (aut√≥matas) y optimizaci√≥n no convexa.*

15. **Teor√≠a de grafos para ciencia de datos**
* Matrices de adyacencia, conectividad, caminos, BFS/DFS.
* **Aplicaci√≥n en DS/ML:** Network science (PageRank, comunidades), recomendadores, detecci√≥n de fraude.
* **Bibliograf√≠a:** **[Benjamin‚ÄìChartrand‚ÄìZhang]**, **[Kumar]**.


16. **Espectros de grafos y geometr√≠a (Spectral Graph Theory)**
* Espectro del Laplaciano; autovalores y autovectores en grafos.
* **Aplicaci√≥n en DS/ML:** Spectral clustering, Graph Embeddings, fundamentos de GNNs.
* **Bibliograf√≠a:** **[Kumar]**, **[ESL/Bishop (ref. espectral)]**.


17. **Modelos sobre grafos y unificaci√≥n**
* Markov blankets, MRF (Markon Random Fields) y Factor Graphs.
* Inferencia aproximada (Loopy Belief Propagation).
* **Aplicaci√≥n en DS/ML:** CRFs (secuencias), segmentaci√≥n de im√°genes, denoising.
* **Bibliograf√≠a:** **[Koller & Friedman]**, **[Bishop, Ch. 8]**.


18. **Algoritmos evolutivos y programaci√≥n gen√©tica (GP)**
* Optimizaci√≥n sin gradiente en paisajes no convexos.
* GP: B√∫squeda en espacios de programas (*Symbolic Regression*).
* **Aplicaci√≥n en DS/ML:** AutoML, descubrimiento de ecuaciones interpretables, Neuroevolution.
* **Bibliograf√≠a:** **[Material de C√°tedra: Evolutivos/GP]**.


19. **Aut√≥matas celulares (CA) y din√°mica local**
* Reglas locales ‚Üí Emergencia global. Din√°mica en grillas.
* **Aplicaci√≥n en DS/ML:** Simulaci√≥n de propagaci√≥n (epidemias, tr√°fico), modelos generativos discretos.
* **Bibliograf√≠a:** **[Material de C√°tedra: Aut√≥matas Celulares]**, **[Wolfram (ref. conceptual)]**.

##  C√≥mo utilizar este material (Instalaci√≥n y Uso)

Tienes dos formas principales de consumir y ejecutar las clases de este repositorio:

### Opci√≥n A: En la Nube (Recomendado)
No necesitas instalar nada en tu computadora.
1. **Simuladores Interactivos:** Haz clic en los botones azules `[![Ver en GitHub]]` de la tabla superior para acceder a los archivos `.html`. Puedes verlos funcionar directamente usando los botones verdes de `[![Ver Simulaci√≥n Interactiva]]`.
2. **Jupyter Notebooks:** Haz clic en los botones de **Binder** o **Colab** que se encuentran en la parte superior. Esto abrir√° un entorno virtual en tu navegador donde podr√°s ejecutar el c√≥digo Python celda por celda.

### Opci√≥n B: Ejecuci√≥n Local
Si prefieres tener los archivos en tu m√°quina:
1. Clona este repositorio:
   ```bash
   git clone [https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos.git](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos.git)

---

## üìö Bibliograf√≠a General del Curso

* **Wasserman** ‚Äî *All of Statistics*.
* **Hastie, Tibshirani, Friedman** ‚Äî *The Elements of Statistical Learning (ESL)*.
* **Efron & Hastie** ‚Äî *Computer Age Statistical Inference (CASI)*.
* **Bishop** ‚Äî *Pattern Recognition and Machine Learning (PRML)*.
* **Koller & Friedman** ‚Äî *Probabilistic Graphical Models*.
* **Gelman et al.** ‚Äî *Bayesian Data Analysis (BDA3)*.
* **Pearl** ‚Äî *Causal Inference in Statistics*.
* **Shumway & Stoffer** ‚Äî *Time Series Analysis and Its Applications*.
* **Mardia, Kent, Bibby** ‚Äî *Multivariate Analysis*.
* **Benjamin, Chartrand, Zhang** ‚Äî *The Fascinating World of Graph Theory*.

## üß± Estructura del repositorio (high-level)

> *Ajust√° estos nombres si difieren en tu repo.*

- `docs/` ‚Äî sitio MkDocs / GitHub Pages (material por m√≥dulo)
- `src/classroom/` ‚Äî notebooks, scripts y pr√°cticas por m√≥dulo
- `assets/` ‚Äî im√°genes, gifs y recursos visuales del curso
- `tests/` ‚Äî tests (si aplica)
- `requirements.txt` / `pyproject.toml` ‚Äî dependencias

---

## üõ†Ô∏è Uso y ejecuci√≥n

### 1) Clonar el repositorio

```bash
git clone [https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos.git](https://github.com/sgevatschnaider/estadisticas-para-ciencia-de-datos.git)
cd estadisticas-para-ciencia-de-datos

```

### 2) Crear entorno e instalar dependencias (Python)

Se recomienda usar un entorno virtual para evitar conflictos.

```bash
# Crear entorno virtual
python -m venv .venv

# Activar entorno (Linux/macOS):
source .venv/bin/activate

# Activar entorno (Windows):
# .venv\Scripts\activate

# Instalar dependencias
pip install -U pip
pip install -r requirements.txt

```

### 3) Calidad de c√≥digo (opcional, recomendado)

```bash
pre-commit install
pre-commit run --all-files

```

### 4) Ejecutar notebooks / pr√°cticas

Si trabaj√°s con **Jupyter Lab**:

```bash
jupyter lab

```

Si us√°s **VSCode**:

> Abre la carpeta del repositorio y ejecuta los archivos `.ipynb` directamente usando la extensi√≥n de Jupyter.

---

## ‚úÖ Requisitos y nivel esperado

* **Matem√°tica:**
* Probabilidad y estad√≠stica cl√°sica (intervalos, tests b√°sicos, ANOVA).
* √Ålgebra lineal (vectores, matrices, autovalores/autovectores).


* **Programaci√≥n:**
* **Python** (lenguaje principal).
* *R / Julia* (como soporte opcional).



## ü§ù Contribuciones

Issues y Pull Requests son bienvenidos.

* Si vas a proponer cambios grandes (estructura, syllabus o tooling), por favor abr√≠ primero un **Issue** describiendo el objetivo y el impacto.

## ‚öñÔ∏è Licencia

Este material se distribuye bajo licencia **MIT**. Ver el archivo [LICENSE](https://www.google.com/search?q=LICENSE) para m√°s detalles.



