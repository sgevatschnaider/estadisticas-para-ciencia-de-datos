# Estad√≠sticas para Ciencia de Datos ‚Äî Classroom

# Applied Statistics for Data Science (Engineering & Research Track)

[![src/classroom/probabilidad](https://img.shields.io/badge/src-probabilidad-0366d6)](https://github.com/USUARIO/Estadisticas-Ciencia-de-Datos-Classroom/tree/main/src/classroom/probabilidad)
[![src/classroom/inferencia](https://img.shields.io/badge/src-inferencia-0366d6)](https://github.com/USUARIO/Estadisticas-Ciencia-de-Datos-Classroom/tree/main/src/classroom/inferencia)
[![src/classroom/learning](https://img.shields.io/badge/src-learning-0366d6)](https://github.com/USUARIO/Estadisticas-Ciencia-de-Datos-Classroom/tree/main/src/classroom/learning)
[![src/classroom/pgm](https://img.shields.io/badge/src-pgm-0366d6)](https://github.com/USUARIO/Estadisticas-Ciencia-de-Datos-Classroom/tree/main/src/classroom/pgm)
[![src/classroom/tiempo](https://img.shields.io/badge/src-tiempo-0366d6)](https://github.com/USUARIO/Estadisticas-Ciencia-de-Datos-Classroom/tree/main/src/classroom/tiempo)

<p align="center">
  <a href="https://USUARIO.github.io/Estadisticas-Ciencia-de-Datos-Classroom/" target="_blank" rel="noopener">
    <img alt="Live docs ‚Äî GitHub Pages" src="https://img.shields.io/badge/Live%20docs-GitHub%20Pages-2b3137?style=for-the-badge&logo=github" />
  </a>
  &nbsp;
  <a href="#" target="_blank" rel="noopener">
    <img alt="Live demos ‚Äî Binder" src="https://img.shields.io/badge/Live%20demos-Binder-f5a250?style=for-the-badge&logo=jupyter" />
  </a>
</p>

Material docente de nivel **Posgrado / Ingenier√≠a Avanzada** para Ciencia de Datos.

Este curso asume conocimientos previos de estad√≠stica cl√°sica (ANOVA, Tests de Hip√≥tesis b√°sicos) y se centra en la **"Caja Blanca"** de la ingenier√≠a moderna: inferencia computacional, geometr√≠a de los datos, regularizaci√≥n y modelos gr√°ficos probabil√≠sticos.

<p align="center">
  <a href="https://www.python.org/">
    <img alt="Python" src="https://img.shields.io/badge/python-3.10%2B-blue">
  </a>
  <a href="https://www.r-project.org/">
    <img alt="R" src="https://img.shields.io/badge/R-4.x-276DC3">
  </a>
  <a href="https://julialang.org/">
    <img alt="Julia" src="https://img.shields.io/badge/Julia-1.x-9558B2">
  </a>
  <a href="https://github.com/psf/black">
    <img alt="Code style: black" src="https://img.shields.io/badge/code%20style-black-000000.svg">
  </a>
  <a href="LICENSE">
    <img alt="License: MIT" src="https://img.shields.io/badge/License-MIT-blue.svg">
  </a>
</p>

---

## üìö Bibliograf√≠a Base (The Canon)

Las referencias bibliogr√°ficas en el programa corresponden a las siguientes obras maestras:

* **[Wasserman]**: *All of Statistics: A Concise Course in Statistical Inference* (Larry Wasserman).
* **[ESL]**: *The Elements of Statistical Learning* (Hastie, Tibshirani & Friedman).
* **[BDA3]**: *Bayesian Data Analysis, 3rd Ed.* (Gelman, Carlin, et al.).
* **[Efron]**: *Computer Age Statistical Inference* (Efron & Hastie).
* **[Pearl]**: *Causality: Models, Reasoning, and Inference* (Judea Pearl).
* **[Shumway]**: *Time Series Analysis and Its Applications* (Shumway & Stoffer).

---

## üó∫Ô∏è Syllabus Detallado

### M√ìDULO I: PROBABILIDAD MULTIVARIANTE Y GEOMETR√çA
*Prerrequisito: Se asume dominio de variables aleatorias univariadas y distribuciones est√°ndar.*

1.  **Vectores Aleatorios y Geometr√≠a de Datos**
    * Notaci√≥n matricial: Vector de medias $\mu$ y Matriz de Covarianza $\Sigma$.
    * Dependencia lineal vs. Independencia estad√≠stica.
    * *Bibliograf√≠a:* **[Wasserman, Ch. 3]**, **[ESL, Ch. 2.4-2.5]**.
2.  **La Normal Multivariante (MVN)**
    * Definici√≥n y propiedades geom√©tricas (elipsoides de densidad).
    * Descomposici√≥n Espectral (Eigenvalues) y blanqueo de datos (Whitening).
    * La MVN como base para Gaussian Processes y PCA.
    * *Bibliograf√≠a:* **[ESL, Ch. 4.3]**, **[Wasserman, Ch. 14]**.
3.  **Teor√≠a del Aprendizaje (Concentraci√≥n de la Medida)**
    * Revisi√≥n de LLN y CLT desde la convergencia estoc√°stica.
    * Desigualdades de **Chebyshev** y **Hoeffding**: ¬øPor qu√© aprenden las m√°quinas? (Cotas de error).
    * *Bibliograf√≠a:* **[Wasserman, Ch. 4-5]**.

### M√ìDULO II: INFERENCIA COMPUTACIONAL Y "MODERNA"
*Enfoque: Superar las limitaciones de las pruebas de hip√≥tesis de "tabla" mediante simulaci√≥n.*

4.  **Teor√≠a Asint√≥tica y M√°xima Verosimilitud (MLE)**
    * El MLE como problema de optimizaci√≥n.
    * Propiedades: Consistencia, Eficiencia y Normalidad Asint√≥tica.
    * Informaci√≥n de Fisher y Cota de Cram√©r-Rao (L√≠mite de precisi√≥n).
    * *Bibliograf√≠a:* **[Wasserman, Ch. 9]**.
5.  **El Bootstrap y M√©todos de Resampling**
    * Estimaci√≥n del Error Est√°ndar sin f√≥rmulas cerradas.
    * Bootstrap Param√©trico vs. No Param√©trico. Intervalos de confianza BCa.
    * *Bibliograf√≠a:* **[Wasserman, Ch. 8]**, **[Efron, Ch. 10-11]**.
6.  **Tests de Hip√≥tesis en Alta Dimensionalidad**
    * Test de Wald, Score y Likelihood Ratio Test (LRT).
    * El problema de las comparaciones m√∫ltiples: P-Hacking.
    * Correcci√≥n de Bonferroni y False Discovery Rate (FDR - Benjamini-Hochberg).
    * *Bibliograf√≠a:* **[Wasserman, Ch. 10]**, **[Efron, Ch. 15]**.

### M√ìDULO III: APRENDIZAJE ESTAD√çSTICO (REGRESI√ìN AVANZADA)
*Enfoque: Trade-off Sesgo-Varianza y selecci√≥n de modelos.*

7.  **Geometr√≠a de M√≠nimos Cuadrados (OLS)**
    * Regresi√≥n como proyecci√≥n ortogonal en subespacios lineales.
    * Teorema de Gauss-Markov.
    * Diagn√≥stico: Leverage, Distancia de Cook y An√°lisis de Residuos.
    * *Bibliograf√≠a:* **[ESL, Ch. 3.2]**, **[Wasserman, Ch. 13]**.
8.  **Regularizaci√≥n y Selecci√≥n de Modelos**
    * La maldici√≥n de la dimensionalidad ($p > n$).
    * **Ridge Regression (L2):** Contracci√≥n de coeficientes y priors Gaussianos.
    * **Lasso (L1):** Sparsity y selecci√≥n de variables (priors de Laplace).
    * Criterios de Informaci√≥n: AIC, BIC y Mallows‚Äô Cp.
    * *Bibliograf√≠a:* **[ESL, Ch. 3.4 & Ch. 7]**.
9.  **Modelos Lineales Generalizados (GLM)**
    * Familia Exponencial de distribuciones.
    * Funci√≥n de enlace (Link function): Log√≠stica (Clasificaci√≥n) y Poisson (Conteos).
    * Algoritmo IRLS (Iteratively Reweighted Least Squares).
    * *Bibliograf√≠a:* **[ESL, Ch. 4.4]**, **[McCullagh & Nelder (Cl√°sico) / Wasserman Ch. 13]**.

### M√ìDULO IV: MODELOS ESTRUCTURADOS, GRAFOS Y TIEMPO
*Enfoque: Modelar dependencias complejas y causalidad.*

10. **Probabilistic Graphical Models (PGMs)**
    * **DAGs:** Grafos Ac√≠clicos Dirigidos y factorizaci√≥n de la conjunta.
    * Independencia Condicional y **D-Separation** (lectura de grafos).
    * Plate Notation para modelos jer√°rquicos.
    * *Bibliograf√≠a:* **[Wasserman, Ch. 17]**, **[Bishop, Ch. 8]**.
11. **Inferencia Causal**
    * Correlaci√≥n vs. Causaci√≥n: La escalera de Pearl.
    * Intervenciones: El operador $do(x)$.
    * Confounders, Colliders y el criterio "Back-door".
    * *Bibliograf√≠a:* **[Pearl, Ch. 1-3]**, **[Wasserman, Ch. 19]**.
12. **Inferencia Bayesiana y MCMC**
    * Priors Conjugados vs. No Informativos.
    * La Posterior como distribuci√≥n de creencias.
    * Muestreo: Introducci√≥n a Markov Chain Monte Carlo (Metropolis-Hastings).
    * *Bibliograf√≠a:* **[BDA3, Ch. 1-3]**, **[Wasserman, Ch. 11]**.
13. **Series Temporales (Grafos Din√°micos)**
    * Procesos Estoc√°sticos: Estacionariedad y Autocorrelaci√≥n.
    * Modelos ARIMA.
    * Modelos de Espacio de Estados (SSM) y Filtro de Kalman.
    * *Bibliograf√≠a:* **[Shumway, Ch. 1-3 & Ch. 6]**.

---

## üõ†Ô∏è Uso y ejecuci√≥n

### 1) Clonar e instalar dependencias

```bash
git clone [https://github.com/USUARIO/Estadisticas-Ciencia-de-Datos-Classroom.git](https://github.com/USUARIO/Estadisticas-Ciencia-de-Datos-Classroom.git)
cd Estadisticas-Ciencia-de-Datos-Classroom

# Crear entorno virtual (Recomendado)
python -m venv .venv
source .venv/bin/activate  # En Windows: .venv\Scripts\activate

# Instalar dependencias
pip install -U pip
pip install -r requirements.txt

# Instalar hooks de git (para calidad de c√≥digo)
pre-commit install
